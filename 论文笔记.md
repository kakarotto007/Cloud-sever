

# 模板

## 《CLAP: Learning Audio Concepts From Natural Language Supervision》

### 0 Abstract

---

### 1 Introduction

---

### 2 Method

---

### 3 Experiment

---

### 4 Result and Discussion

---

### 5 Conclusion



# DOA

## 《Adaptation of Multiple Sound Source Localization Neural Networks with Weak Supervision and Domain-adversarial Training》

深度学习的声源定位方法有两个主要缺点

1. 标记数据获取带有注释的真实数据是一项艰巨的任务。
2. 它们对训练和测试条件之间的不匹配的敏感性。

## 《A SURVEY OF SOUND SOURCE LOCALIZATION WITH DEEPLEARNING METHODS》

> 尽管多年来它们在该领域的显着进步，但它们在噪声、混响和几个同时发射声源可能存在的困难但常见的场景中表现不佳

Although they have shown notable advances in the domain over the years, they are known to perform poorly in difficult yet common scenarios where noise, reverberation, and several simultaneously emitting sound sources may be present.

> 基于深度学习的声源定位系统比传统的声源定位方法好

Most of the reported works have indicated the superiority of DNN-based SSL methods over conventional (i.e., SP-based) SSL methods. 

> General pipeline of a DL-based SSL system.

![image-20231010144807698](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231010144807698.png)

---

**Acoustic environments**

1. 回响
2. 噪声（记录设备的缺陷可能是另一个噪声源）

**Source types**

1. speech（用于 语音增强和语音识别）
2. DCASE上的数据集（虽然他们有时候overlap，但是不一定困难，这是因为他们的频谱特征有明显的差别）

---

**Conventional SSL methods reasons** 

1. 传统的SSL方法经常被用作基于DL的方法的基线；

2. 许多基于DL的SSL方法使用用传统方法提取的输入特征。

**Conventional SSL methods** 

1. GCC-PHAT
2. SRP-PHAT
3. 基于子空间的方法（MUSIC、ESPRIT）
4. ICA

---

**Neural network architectures for SSL**

> Feedforward neural networks（前馈神经网络）（MLP）

复值的mlp比实值的mlp要好

输入特征是IThe interaural time difference (ITD) and the interaural level difference (ILD) 和GCC-PHAT

可以把它当作分类或者是回归问题

4层隐藏层的MLP与4层cnn有相似的结果

==针对不对频率子带（协方差矩阵的特征向量的频率）设计了不同的MLP，每个MLP有8层隐藏层。这是假设每个频率子带有一个声源主导的。全连接层之后是输出，输出是与频率相关的==

弱监督的MLP，因为损失函数没有用到ground truth DOA 标签

---

> Convolutional neural networks

M个麦克风，利用麦克风的相位相关性最好的卷积层数是M-1

网络的末端被分成两个卷积分支，一个用于方位估计，另一个用于语音/非语音信号分类。

也可以把多通道的原始音频当作输入。

mask估计doa，对TF图做mask。A method based on mask estimation was proposed by Zhang et al. [2019b], in which a TF mask was estimated and used to either clean or be appended to the input features, facilitating the DoA estimation by a CNN.

利用单个源训练出的系统就可以对多声源进行定位：. Fahim et al. [2020] applied an 8-layer CNN to the so-called modal coherence of first-order Ambisonics input features (see Section 5) for the localization of multiple sources in a reverberant environment. They proposed a new method to train a multi-source DoA estimation network with only single-source training data, showing an improvement over the system of Chakrabarty and Habets [2019b], especially for signals with three speakers. 

深度可分离卷积有更高的准确率以及更小的模型复杂度。

Bologni等人[2021]提出了一种神经网络架构，包括一组用于逐帧特征提取的2D卷积层，然后是用于时间聚合的时间维度上的几个1D卷积层。《Acoustic Reflectors Localization from Stereo Recordings Using Neural Networks》

输入特征是SPR-PHAT

空洞卷积会减少计算量并且会使最佳的cnn层数减小。

---

> Recurrent neural networks

TF掩码来增强信号

---

> Convolutional recurrent neural networks

两个阶段的神经网络，已经验证通过生成spatial pseudo-spectrum的中间输出对声源定位有帮助。

加入高斯噪声可以让神经网络更加鲁棒。

数据增强和后处理。

通过加入更多卷积层，并且减少max-pooling可以减少层传递之间的信息损失。

把传统卷积替换为GLU后，可以更好的提取相位相关的特征。

加入TCN后考虑到了时间上下文信息，就可以提升定位准确率。

---

> Residual neural networks

> Attention–based neural networks

加入attention之后更能提取时间相关的信息

cross-modal attention(CMA)models,  cross-attention

> Encoder-decoder neural networks

**Autoencoder:**

对抗训练，从仿真环境下训练到真实环境：《Data-efficient framework for real-world multiple sound source 2D localization》，他们引入了一种新的显式变换层，帮助网络对麦克风阵列布局保持不变。训练鉴别器网络将传入的数据标记为合成或真实，并学习欺骗鉴别器的生成器网络。这使得 DoA 估计网络的适应能够从真实数据中推断出。

seld的输入是原始音频：《SoundDet: Polyphonic Moving Sound Event Detection and Localization from Raw Waveform》

一个encoder对应着两个deconders：《SSLIDE: sound source localization for indoors based on deep learning》第二个分支的decoder可以让网络更加的鲁棒。

encoder-decoder输出GCC的TDOA，表现比经典的TDOA好。

**Variational autoencoder**

VAR通常应用于无监督学习，VAE可以看作是AE的概率版本，与经典AE不同，VAE学习解码器输出端数据的概率分布，并对瓶颈层所谓潜在向量的概率分布进行建模，这使得VAE与无监督表示学习的概念紧密相连，因此，解码器可以通过对这些分布进行采样来获得新的数据。

《Semi-Supervised Source Localization in Reverberant Environments With Deep Generative Modeling》他们的VAE由卷积层组成，经过训练以生成麦克风间RTF的相位（见第5.1.1节），通过RTF相位估计说过者的DOA。这里用无标签的RTF和有标签的数据进行半监督学习。

**U-Net architecture**

一开始在图像分割中被提出来，在U-net中，输入特征在整个编码器层被分解为连续的特征图，然后在整个解码器层被重新组合为“对称”特征图，类似于CNN。在编码器和解码器中具有相同级别的特征图的相同维度使得能够经由残差连接将信息从编码器级别直接传播到解码器的相应级别。这导致了典型的U形示意图。残差连接用于将一个编码器输出连接到同一阶段解码器的输入，以缓解损失信息问题。

《Dynamically locating multiple speakers based on the time-frequency domain》多移动声源

《The cone of silence: speech separation by localization》输入是原始音频

《DCASE2020TASK3:ASINGLESTAGEFULLYCONVOLUTIONALNEURALNETWORK FORSOUNDSOURCELOCALIZATIONANDDETECTION》输入是log-mel+gcc-phat  输出是doa和sed

TASLP：《Source Localization Using Distributed Microphones in Reverberant Environments Based on Deep Learning and Ray Space Transform》，第一阶段用来生成GCC-PHAT，第二阶段采用U-net

### Input features

所考虑的特征可以是低级信号表示，例如波形或频谱图、手工制作的特征，例如双耳特征，或者 MUSIC 或 GCC-PHAT。

### Output strategies

分类和回归，也可以用神经网络估计中间变量，然后用传统的声源定位算法估计doa。

### Data augmentation techniques

《A Four-Stage Data Augmentation Approach to ResNet-Conformer Based Acoustic Modeling for Sound Event Localization and Detection》

1. changing the location of the sources by swapping audio channels.
2. The second method is based on the extraction of spatial and spectral information on the sources, which are then modified and recombined to create new training exampless
3. The third one relies on mixing multiple examples, resulting in new multi-source labelled mixtures. 
4. The fourth technique is based on random TF masking.

### Learning strategies

该网络使用标记数据训练进行预训练，并使用无监督学习进行细化(或微调)。在SSL中已经提出了半监督学习来提高神经网络在监督训练或真实数据看不见的条件下的性能，与仅在监督方式训练时的性能相比。这可以看作是丰富尺寸或条件过有限标记训练数据集的替代方法。

### Conclusions and perspectives

#### Adaptation to (limited sets of) real-world data

## 《SOUND EVENT LOCALIZATION BASED ON SOUND INTENSITY VECTOR REFINED BY DNN-BASED DENOISING AND SOURCE SEPARATION》

### 0 Abstract

基于DNN的声源定位方法在重叠的声源条件下不如基于物理的方法。

所以在DNN的基础上用声源强度矢量做了细化。

---

### 1 Introduction

MUSIC方法可以准确估计多源DOA的位置、IV-based方法有很好的时间分辨率，但是两种方法在低信噪比的情况下效果会下降。

所以一个 Speech Enhancement 模块来去噪，后面跟一个physics-based DOA estimation procedure.

## 《Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking》

### 0 Abstract

受时频掩蔽在语音增强和语音分离中的成功启发，本文提出了在卷积神经网络中更好地利用时频掩蔽的新方法，以提高定位的鲁棒性。

1. 首先，开发了掩模估计网络，通过将估计的掩模附加或乘以原始输入特征来辅助DOA估计。
2. 进一步提出了一种多任务学习架构来联合优化掩模和DOA估计网络，并对两种模式进行了设计和比较。

---

### 1 Introduction

输入特征中的大多数是在时间-频率（T-F）域中导出的，并且它们仍然容易受到失真的影响，因为并非所有的T-F仓都由目标语音主导。这项工作旨在通过消除特征中大多数以噪声为主的T-F仓来提高鲁棒性，以最大限度地减少噪声和混响的影响。

mask的目的是为了消除噪声以及混响。

---

### 2 Problem Description

**相位信息**是DOA估计任务的本质。

---

### 3 CNN Based DOA Estimation with T-F Masking 

两个网络，一个掩码估计网络，一个estimation网络。

![image-20231031155113362](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031155113362.png)

mask网络估计的是每个TF仓是目标语音信号的概率。然后乘以（或者相加）原始的输入得到现在的输入，目的是为了减少噪声以及回响的影响。

![image-20231031161613389](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031161613389.png)

## 《DATA-EFFICIENT FRAMEWORK FOR REAL-WORLD MULTIPLE SOUND SOURCE 2D LOCALIZATION》

### 0 Abstract

音频数据集短缺，并且训练时需要所有类型的麦克风阵列；在合成数据集上训练，然后在真实环境下测试，因为域不匹配效果会不好。

我们提出了对抗学习的方法来缩减synthetic and real的gap

---

### 1 Introduction

为了解决域偏移的问题，大组会分享过一篇：通过弱监督（只有源的个数）和梯度反转训练（领域对抗神经网络(DANN)、没有提升效果）

对抗方法在其它任务上表现很好，由于尚不清楚对抗性方法是否适用于 SSL，因此我们展示了使用此类方法的实验的结果。

DNN的声源定位方法的好处：1. 能够适应具有挑战性的声学条件，但是有域偏移问题。2.  可以被训练来与各种麦克风阵列布局使用，但是只在用于训练的数据集的麦克风阵列表现好。

Contribution：

1. 域转移：我们研究了两种基于对抗性学习的域适应方法：梯度反转和标签翻转
2. 泛化到看不见的布局配置的难度：它允许网络明确使用麦克风阵列之间的相对姿态，这反过来又使网络在推理过程中更好地推广到看不见的布局中的数据。

---

### 2 DOMAIN ADAPTATION FOR MULTIPLE SOUND SOURCE 2D LOCALIZATION

这扩展了源域，希望覆盖目标域，例如，可以对诸如噪声和混响之类的参数进行随机化以实现这种效果。假设我们无法再改进模拟器，并且模型在实际数据上的性能仍然不足，则可以使用领域自适应方法。

有 source data 和label， target data。 没有target 的label。所以是一个无监督域适应问题。

---

### 3 EXPLICIT TRANSFORMATION LAYER

---

### 4 EXPERIMENTS

---

### 5 Conclusion

## 《FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization》

### 0 Abstract

在不利环境下提取 direct-path spatial features（直接路径空间特征）很重要。本片论文设计了一个full-band和narrow-band的网络来估计direct-path inter-channel phase difference（DP-IPD）。分别学习全波段相关性和窄带提取。

---

### 1 Introduction

定位的特征与direct-path signal propagation相关。

DNN的输入可以是signal-level 的特征也可以是feature-level的特征。

提出了FN-SSL network估计DP-IPD，输入的还是多通道的原信号。

---

### 2 Method

---

### 3 Experiments

---

### 4 Conclusions

# Audio-Visual 

## 《Visual Sound Localization in the Wild by Cross-Modal Interference Erasing》-AAAI

#### Abstract

屏幕外的声音以及背景噪声会影响定位，key idea是通过重新定义和雕刻有区别的音频表示来消除干扰，通过跨模态蒸馏的跨模态参考器模块来擦除  可以听到的屏幕外的声音以及静默但可见物体的影响。

#### 1 Introduction

《Multiple Sound Sources Localization from Coarse to Fine》：从粗粒度到细粒度解决问题，但是依赖于音频类的标签

《Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching》在干净的声场环境下，两阶段的策略，抑制不发声声源的影响。

Motivation：多声源造成的挑战：1）在定位任务中，模型只趋向于去定位声音大的声源，需要更多的音频表示。2）看不见的声源以及背景噪声会对视听定位造成影响。

在paper中，提出了一个无标签的方法叫做Interference Eraser。

contributions：1）设计了一个Audio-Instance-Identifier来擦除声音幅值在不均匀音频混合中对判别音频感知的影响。2）设计了跨模态参考模块来消除干扰

#### 2 Related Work

> Audio-Visual Correspondence Learning

音频和视频之间的对应关系为多模态场景下的学习提供了自然和丰富的自我监督。

其他论文：1）生成视听相关图 — 计算每个空间网格的全局音频嵌入以及视觉特征的相似度。2）采用试听一致性来对齐通道跨模态分布。

前面提到的都是针对一个声源的情况。

> Visual Sound Source Localization

看不懂了

# SELD

## 《ZERO- AND FEW-SHOT SOUND EVENT LOCALIZATION AND DETECTION》

### 0 Abstract

已知可以使用 contrastive language-audio pretraining（CLAP）生成声音时间类别以及对应的标签，但是对于SELD任务还需要对给音频分配给相应的DOA

motivation：为了解决DOA的分配问题。

contributions：提出了一种embedd-ACCODA模型，该模型被训练为输出按轨迹的CLAP嵌入和相关的ACCDOA。

1. embed-ACCDOA model，输出是 track-wise CLAP embedding 和 ACCDOA
2. straightforward combination of the CLAP audio encoder and a DOA estimation model
3. combination of the embed-ACCDOA model 和 CLAP audio encoder

### 1 Introduction

zero-shot意味着只有声音事件的文本样本

few-shot意味着一些声音事件的音频样本

we propose an **embed-ACCDOA model** that is trained to output track-wise CLAP embedding and the corresponding ACCDOA vector.  In the inference, if activity from the estimated ACCDOA vector is larger than a threshold, the system outputs DOA from the ACCDOA vector and a class whose support embedding is the nearest to the estimated embedding。

Train the embed-ACCDOA model with a synthetic dataset, then evaluate the system with two datasets:: Sony-TAU Realistic Spatial Soundscapes 2023 (STARSS23)and TAU-NIGENS Spatial Sound Events 2021 (TNSSE21)

### 2 ZERO-AND FEW-SHOT SELD TASKS

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025144053420.png" alt="image-20231025144053420" style="zoom:80%;" />

$E_{c}^{t}，E^{q'}\in \mathbb R^D$, 在SELD任务中保留$E_{c}^{t}$。 与SED任务不同的是 

1. 需要得到每一个时刻的结果
2. 需要DOA
3. 需要考虑重叠的声音事件

所以把 $E^{q'}的维度升到\mathbb R^{D \times N \times T}$。N是轨道维度，T是时间维度.同时与DOA vectors相关联 $R^q \in \mathbb R^{3 \times N \times T}$。 对于每个audio query embedding $E_{nt}^{q}$，在$E^t$里找到最相似的$E_{c}^{t}$。

![image-20231025145311559](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025145311559.png)

在k-shot的时候，用audio embedding 替换掉了 text embedding，不太懂。。。

### 3 METHOD

#### 	3.1 Embed-ACCDOA model



## 《Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation》

### 0 Abstract

同时学习 spectral（光谱的）和 spatial （空间的）的特征。

提出了multi-level数据增强pipeline：waveforms，Mel spectrograms， GCC特征。

此外还提出了一种通道随机swap，它是simple and effective的，防止为了在特定通道的过拟合。

通过使用数据增强，他们发现线性层比有监督的学习效果要好。

### 1 Introduction

**缺少音频信息的注释**使得大规模有监督训练不能实现，所以提出了第一个多通道的自监督表示方法， a simple multi-channel framework for contrastive learning (MC-SimCLR) 是 a simple framework for contrastive learning的 adaptation.

---

### 2 Related Works

自监督学习不需要明确的标签，是一种很有前途的表征学习方法。学习到的表示通常用作下游任务的输入特征，减少了对大量标记训练数据的需求，并且提高了任务性能。

#### 	2.1 SED领域

- Bade on SimCLR，通过对比学习最大化相同片段的相似度，最小化不同片段的相似度
- Self-distrllation 方法自蒸馏方法通过让在线编码器预测目标编码器的嵌入，在不对比多个片段的情况下实现了相同的目标。
- 此外，transformer补丁建模方法还通过代理任务从未掩蔽的频谱图补丁中预测和重建掩蔽的谱图补丁来学习事件分类的判别特征。

#### 	2.2 DOA领域

《Sound Localization by Self-Supervised Time Delay Estimation》通过对比学习，但是对声源数量有限制。

---

### 3 MC-SIMCLR

为了有效地从未标记的多通道音频中捕捉这种相似性，MC-SimCLR提取光谱和空间特征，对各种特征和维度进行增强，并在训练过程中优化对比目标。

#### 	3.1 Input Features

**提取waveforms：**We crop two random patches, denoted as ${x_i},{x_j}\in{\mathbb{R} }_{}^{M\times N} $ , 形成一对正样本。 一段waveform是M channels和持续一秒的音频。不够一秒的用 0 paded ， 两个patch可能有重叠。

**提取Mel和GCC特征**: ${X}^{MEL}\in \mathbb{R}^{M\times F \times T}$  ，${X}^{GCC}\in \mathbb{R}^{M/2\times F \times T}$

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231023155753078.png" alt="image-20231023155753078" style="zoom:80%;" />

最后拼接特征在通道维度上，一共十个channels：4个Mel spectrograms 和 6个GCC features

#### 	3.2 Multi-level Data Augmentation

以前单通道的data augmentation： Mixup、 RandomResizeCrop 、 SpecAugment 、 Compression 都是主要在Mel 图上的。

本次pipeline：

1. we apply **Mixup** to multi-channel **waveforms instead of Mel spectrograms** and RandomResizeCrop concurrently **on all channels of both Mel and GCC features.**
2. Furthermore, we introduce novel channelwise data augmentation methods **ChannelSwap**, which operates on the order of the microphones, and **ChannelDrop**, which randomly **masks** Mel and GCC channels.

我们描述了应用于输入数据的序列中的每种数据增强方法，**从波形开始，在到达编码器之前通过Mel和GCC特征进行。**以类似的方式集成额外的数据扩充具有灵活性。

#### 	3.3 ChannelSwap

重新排列圆形或四面体麦克风阵列中波形的麦克风顺序，从而产生新的记录，在改变空间位置的同时保持频谱特性，而不需要额外的记录或模拟。

如果是4通道的圆形麦克风阵列，对应着8钟通道重排：翻转（-1和1）和旋转（0.90.-90.180），在有监督的训练模式下可以产生八倍多的音频。

但是在我们的情况下，尽管我们不知道ChannelSwap之前或之后的方位值，但我们注意到，ChannelSwap之前方向相似的两个样本（即正对）在操作之后应该继续表现出相似的方向。因此，我们对一个话语的两个片段应用具有相同但随机排列的ChannelSwap，以生成更多对正样本。

#### 	3.4 Mixup

mix一个目标源x和另外一个随机源y，通过convex结合：

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231023161607146.png" alt="image-20231023161607146" style="zoom:80%;" />

以前都是混合Mel谱图，现在我们混合了多通道波形图，来融合谱图和空间信息同时。值得注意的是，我们在确保目标源的突出度之前和之后执行波形归一化，以抵消任何统计偏移。α一般这是为【0，0.01】

#### 	3.5 RandomResizedCrop

RandomResizedCrop对输入图像执行大小调整和裁剪。在空间音频的背景下，Mel和GCC特征都可以被视为2D图像。RandomResizedCropon在Mel特征上的应用引起了（pitch shifting and time stretching effects.）音高偏移和时间拉伸效应。在GCC特征的情况下，除了时间拉伸之外，它还会在源方向上引起轻微的扰动。这种战略调整有助于生成以紧密相邻的方向为特征的正对，而无需精确了解这些来源的确切位置。

剪裁比例设置为：【0.8，1】、纵横比设置为【0.8，1.25】

#### 	3.6 ChannelDrop

于Mel和GCC通道，我们以小概率p将整个通道随机屏蔽为全零。我们观察到，在没有对所有感兴趣的任务进行强有力的监督的情况下，**该模型倾向于关注单个通道或一小部分通道，**从而导致下游任务的次优解决方案。ChannelDrop通过删除整个频道有效地解决了这种潜在的频道偏见，迫使模型关注所有频道以及Mel和GCC功能。（加入关心a通道，把a通道删除，就增加泛化性）

概率设置为p=0.1

#### 	3.8 Projection Head and Contrastive Loss

正样本对${x_i},{x_j}$，通过特征提取、数据增强、encoder后得到${h_i},{h_j}$ 。然后通过 two-layer perceptron把${h_i},{h_j}$投影到${z_i},{z_j}$.

对比损失定义为如下：

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231023163139809.png" alt="image-20231023163139809" style="zoom:80%;" />

sim是余弦相似度，投影仪P被丢弃用于监督评估，hi和hj是提取的空间嵌入。

> 上面是MC-SimCLR
>
> 下面是feature extraction 和data augmentation的 具体步骤

![image-20231024145433663](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231024145433663.png)

---

### 4 Experiment

#### 	4.1 Data Simulation

数据集：FSDnoisy18k dataset  with gpuRIR toolbox。

**FSDnoisy18**k：包括包括单声道音频源，每个音频源属于20个不同类别中的一个，例如引擎、足迹和吉他。

**房间：**对于每个源，我们模拟了一个房间，其宽度、长度和高度从[3.0,10.0]、[3.0,1.0]和[2.5,4.0]米的范围内均匀采样，混响时间（RT60）从[0.1,1.0]秒均匀采样。

**麦克风阵列：**我们还模拟了一个直径为0.1米的圆形全向麦克风阵列，配备了4个均匀间隔的麦克风。源和阵列被放置在房间内的随机位置，其高度以[0.5,2.0]米为单位选择。为了确保正确放置，它们与房间墙壁和彼此之间的距离至少为0.5米。

**FSDnoisy18k**：包含一个较大的噪声（noisy）训练集（15813个片段/38.8小时）、一个较小的干净（clean）训练集（1772个片段/2.4小时）和一个测试（testing）集（947个片段/1.4小时）。为了验证（validation）目的，从clean中随机选择了200个剪辑。每个实验包括两个步骤：对噪声（noisy）的自我监督预训练（第4.2节）和对干净（clean）的监督评估（第4.3节）。我们报告了模型在测试集（testing）上的性能。

#### 	4.2 Model and Pre-training

encode用的是CRNN，embeddding dimension是128.batch size 是512

我们对编码器进行了500个时期的训练，在前10个时期以线性学习率预热开始，在剩余时期以余弦学习率衰减[30]为0。所有实验均在NVIDIA L40 GPU上进行

#### 	4.3 Supervised Evaluation

- Linear-probing：分别训练了两个线性层，分别用于事件分类和角度预测，encoder冻结。
- Fine-tuning：联合训练了两个线性层，并同时更新encoder。
- Subset Fine-tuning：与微调相同，但我们只使用标记数据的子集。

除了使用128的较小批量和0.01的学习率外，我们遵循与预训练步骤相同的超参数。**除了ChannelSwap之外**，没有使用任何数据扩充，因为它被证明可以提高监督SELD性能[9]。此外，如果微调预先训练的模型，我们将0.1的比例因子应用于编码器的梯度，以防止在clean上过拟合。

---

### 5 Result

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231023170303551.png" alt="image-20231023170303551" style="zoom:80%;" />

灰色的是随机初始化的encoder。

### 6 Conclusion

本研究介绍了MC-SimCLR，一种专门用于多声道音频信号的对比学习框架。MC SimCLR熟练地利用未标记的空间音频记录来提取强大的组合频谱和空间表示。在训练过程中，通过跨越波形、Mel谱图和GCC特征的多层次多维数据增强流水线来增强这种表示。通过这些增强功能，与从头开始的训练相比，MC SimCLR在事件分类和本地化方面取得了显著改进。我**们未来的工作将致力于学习移动或重叠空间声音事件的表现**

## 《A TRACK-WISE ENSEMBLE EVENT INDEPENDENT NETWORK FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION》

### 0 Abstract

In this paper, **a track-wise ensemble event independent network** with a novel **data augmentation** method is proposed. 

---

### 1 Introduction

SELDnet 的局限性在于它无法检测到相同类型但是位置不同的声音事件。为了解决这个问题提出EIN的track-wise的output format。

这次提出的模型用DenseNet 和 Conformer来扩展EINV2。The Conformer structure is employed to learn local and global patterns. The DenseNet structure is utilized to increase the diversity of different models for ensemble.

在推理过程中采用了后处理（post-processing）方法来提高性能：…

data augmentation: 这些数据增强操作被随机采样、分层和组合以产生高度多样性的增强特征。

---

### 2 Method

#### 	2.1 Input Features

FOA的 Log-mel spectrograms 和IV特征作为第一个，SALSA作为第二个特征。

Log-mel谱图首先用于SED，而Log-mel空间中的IV用于DoA估计。FOA包括四个通道的信号，即全向通道w和三个方向通道x、y和z。根据四个通道信号的短时傅立叶变换频谱图来计算Log-mel频谱图，并且根据w与Log-mel空间中的x、y、z的互相关来计算IV.

SALSA由两个主要部分组成：多通道对数线性谱图和归一化主特征向量。

#### 	2.2 Network Architecture

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025162450727.png" alt="image-20231025162450727" style="zoom:80%;" />

#### 	2.3 Data Augmentation

我们随机抽取k个数据增强链，默认k=3，每个扩充链都是由一些随机选择的扩充操作组成的。

数据增强操作包括Mixup、SpecAugment、Random Cutout、rotation of FOA signals（channel swap）

---

### 3 Post Processing（后处理）

#### 	3.1 Track-wise Output Format

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025163919983.png" alt="image-20231025163919983" style="zoom:80%;" />

M是轨道数，K是声音事件数量， $\mathbb O_{S}^{M \times K}$是K个声音事件数量的one hot encoding 编码

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025165759312.png" alt="image-20231025165759312" style="zoom: 80%;" />



#### 	3.2 Track-wise Ensemble Model

因为每个轨道可能对应着不同的声音事件，所以一般的方法比如：TTA，average or weighted ensemble方法不适用于Track-wise.

We propose **a novel post-processing method** named **track-wise ensemble model**. 如下所示：

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231025170308378.png" alt="image-20231025170308378" style="zoom: 80%;" />

这个集成模型的输入是不同单个模型的输出。

---

### 4 Experiments

#### 	4.1 Dataset

用的是L3DASS22 Task2， 被分成了三个子集。 train：600   validation：150   test：150   30秒长的audio。

一共14钟声音事件选取自FSD50K。

#### 	4.2 Hyper-parameters and Evaluation Metrics

sampling frequency： 32 kHz.

Audio clips are segmented to have a fix length of 5 seconds with no overlap for training.

特征： 4通道log-mel图谱   3通道IV特征     4通道 log-linear图谱    3通道SALSA特征 然后拼接为14通道的特征

评价指标：F-score based on the location-sensitive detection.   当声音事件类别准确却DOA误差小于阈值时为 true positive

#### 	4.3 Experimental Results

我们的系统是基于1米的空间误差阈值开发的，并在1米和2米的阈值下进行评估

---

### 5 Conclusion

## 《SoundDet: Polyphonic Moving Sound Event Detection and Localization from Raw Waveform》



# SED

## 《CLAP: Learning Audio Concepts From Natural Language Supervision》

### 0 Abstract

Contrastive Language-Audio Pretraining(CLAP)采用了两个encoder把音频和文本描述映射到一个多模态空间。利用得到的encoder做了16个下游任务，达到了Zero-Shot的SoTA。

---

### 1 Introduction

==motivation：==

**有监督学习：**在这种有限的监督下学习限制了预测看不见的类别的灵活性。

**Self-Supervised Learning ：**SSL将语义知识排除在自然语言之外。

支持Zero-Shot预测的模型可以接受输入音频，并为用户输入的任何类别生成预测分数。Zero-Shot不需要训练阶段，所以没有预定义的类别。为了实现这种灵活性和泛化，模型需要学习声学语义和语言语义之间的关系。

上面两种方法的中间路径是：从自然语言的监督中学习音频概念。我们叫我们的方法 Contrastive Language-Audio Pretraining（CLAP）

==Contribution==

1. 介绍了用128k音频和文本对训练的CLAP模型。
2. 模型能够实现零样本预测，因此它消除了训练和强制预定义类别集的需要，并在推理时实现了灵活的类预测
3. CLAP通过建立零样本SoTA性能，概括为8个域的16个下游任务。

---

### 2 Method

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231024150848000.png" alt="image-20231024150848000" style="zoom:80%;" />

Zero-Shot linear classification 是计算嵌入空间特征的余弦相似度得到的。

#### 	2.1 Contrastive Language-Audio Pretraining

$X_a \in \mathbb R^{F \times T}代表了音频,X_t代表了文本 $.     $\left \{ X_a,X_t\right \}_i \in [0,N].$ audio-text pair in a batch of N 。 为了方便：用$\left \{ X_a,X_t\right \}$

通过audio和text encoder后分别得到了

​																	 <img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231024152511360.png" alt="image-20231024152511360" style="zoom: 67%;" />

然后通过linear projection 得到了

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231024152614454.png" alt="image-20231024152614454" style="zoom:67%;" />

都是Nxd维的。

然后计算相似度的到相似度维度为NxN的相似度矩阵：<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231024152657428.png" alt="image-20231024152657428" style="zoom:67%;" />

最后计算loss：

#### 	2.2 Zero-Shot Linear Classification

text输入的时候有C个，audio输入的时候有一个。通过encoder和linear projection得到了 $1 \times d 的 E_a$，以及$C \times d的E_t$，最后通过计算余弦相似度得到了一个 $1 \times C$ 的相似度矩阵。最后通过softmax、sigmoid 函数得到分类结果。

---

### 3 Experiment

我们使用了来自8个不同领域的16个数据集作为下游任务。

CRNN14作为audio encoder。

BERT作为text encoder。

#### 	3.3 Evaluation setups for CLAP

1. **Zero-shot Evaluation** ： 学习的是CLAP对没有看过的音频和类别的泛化能力。上图所示的右侧就是结构。用prompt代替class label。例如：This is a sound of 【class label】
2. **Supervised Feature Extraction Evaluation**：学习的是CLAP的音频表征能力。把CLAP当做一个特征提取器。在encoder后接1-3个全连接层。
3. **Supervised Finetune Evaluation**：把encoder解冻，其它跟上面一样。

---

### 4 Result and Discussion

#### 	4.1 Zero-Shot results

比随机的好，

#### 	4.2 Supervised results

在五个数据集中达到了SOTA，其他没达到的原因可能是训练数据不够。

#### 	4.3 冻结和不冻结

#### 	4.4 prompt的影响

---

### 5 Conclusion

我们介绍了CLAP，用于从自然语言监督中学习音频概念。CLAP不需要用于训练类标签，实现了灵活的类预测，并可推广到多个下游任务。训练数据由128k个音频-文本对组成，这比类似的计算机视觉模型使用的至少小0.001%。尽管如此，CLAP在零样本性能中建立了SoTA，并在5项任务的监督性能中建立SoTA。因此，CLAP显示出建立音频基础模型的潜力，该模型可以通过自然语言监督进行学习，并可以推广到广泛的任务，同时实现SoTA性能。

# Speech Enhancement

**echo**是指声音在环境中反射后返回到麦克风的现象。当声音源在一个封闭的空间中，声波会碰到墙壁、地板、天花板等表面，然后以不同的角度反射回来。这些反射声会被麦克风捕获到，形成原始声音的延迟复制，产生一个延迟较长的回声信号。回声会导致原始信号的重复和混叠，对语音识别、通信和音频处理等应用产生干扰。

**reverbation**是指声音在空间中反射和散射后产生的持续衰减的效果。当声音源在一个开放的空间中，声波会与房间的墙壁、家具、地板等表面相互作用，并以多个角度和路径进行反射。这些反射声会产生一系列延迟和衰减的信号，形成持续一段时间的余响。混响使得声音变得模糊，失去清晰度和定位感。

回声和混响都是由声音在环境中的反射引起的，但它们的主要区别在于时间和强度方面。回声是由于声音在环境中的反射引起的延迟较长的重复信号，而混响是多个反射声和散射声的叠加，产生持续衰减的效果。

## 《MULTI-SCALE TEMPORAL FREQUENCY CONVOLUTIONAL NETWORK WITH AXIAL ATTENTION FOR SPEECH ENHANCEMENT》

### 0 Abstract

语音质量通常因声学回波、背景噪声和混响而降低。在本文中，我们提出了一个由深度学习和信号处理组成的系统，以同时抑制回声、噪声和混响。

深度学习：speech dense-prediction backbone

信号处理：a linear acoustic echo canceller is used as conditional information for deep learning.

为了提升深度学习的效果：strategies such as **a microphone and reference phase encoder**, **multi-scale time-frequency processing**, and **streaming axial attention** are designed

---

### 1 Introduction

为了抑制声学回波，可以采用一种称为线性声学回波消除器 (LAEC) 的音频处理组件。但是在人的非线性畸变和振动效应的存在效果不好。所以用信号处理以及深度学习结合的方式。

SP：The SP part consists of a **simple time delay compensator** **(TDC) based on** **generalized correlation** [5] and a **LAEC based on the two-echo-path model*****with PNLMS adaptive filter** .

DNN: multi-scale temporal frequency convolutional network with axial self-attention (MTFAA-Net).

contributions:

1. 对于回声消除，我们设计了一种新的SP和DNN组合。与之前的LAEC和DNN串联不同，我们只使用LAEC作为DNN的条件信息，避免了将LAEC引起的失真引入估计的目标语音中。
2. 提出了一种用于各种语音密集预测任务的主干。相位编码器（PE）、多尺度时间频率处理和流轴向自注意力（ASA）旨在提高主干的性能。在PE之后应用基于等效矩形带宽(ERB)的频带合并模块来处理计算复杂度较低的全频带信号.

---

### 2 PROBLEM FORMULATION

![image-20231031102835403](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031102835403.png)

经过LAEC后会得到干净语音、残余回声、混响和背景噪声的混合。

但是还是把Y（t,f）作为输入。

---

### 3 PROPOSED BACKBONE FOR SPEECH DENSE-PREDICTION

![image-20231031103130570](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031103130570.png)



#### 	3.1 Phase Encoder

目的是把复数域映射到实域。PE有三个输入：1. receive the microphone signal    2. the LAEC output 3 .the far-end reference signals

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031104534792.png" alt="image-20231031104534792" style="zoom:67%;" />

#### 	3.2. Band Merging and Splitting（波段合并和拆分）

语音有价值信息在频率维度上的分布是不均匀的，**尤其是对于全频带信号。在高频带中有许多冗余特性。**在高频下合并的特征可以减少冗余。BS是BM的逆过程。在本文中，BM和BS波段是根据ERB尺度来划分的

#### 	3.3. TF-Convolution Module

时间卷积网络（TCN）中使用2D深度卷积（D-Conv）而不是1D D-Conv。**D-Conv也被设计为时间维度上的扩张卷积，可以被视为沿时域的多尺度建模**。TF卷积模块（TFCM）使用的卷积块如图2所示。（b），它由两个逐点卷积（P-Conv）层和一个核大小为（3,3）的D-Conv层组成。扩张范围从1到2B−1的B卷积块连接在一起形成TFCM。**在使用小卷积核的同时，多尺度建模改进了TFCM的感受野。**

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031105252457.png" alt="image-20231031105252457" style="zoom:67%;" />

#### 	3.4. Axial Self-Attention

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031110754762.png" alt="image-20231031110754762" style="zoom:50%;" />

#### 	3.5. Frequency Down and Up Sampling

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031113336489.png" alt="image-20231031113336489" style="zoom:50%;" />

FD和FU采样被设计用于提取多尺度特征。在每个尺度上，TFCM和ASA都用于特征建模，这将提高网络描述特征的能力。

#### 	3.6 Mask Estimating and Applying



### 4 Result and Discussion

---

### 5 Conclusion
