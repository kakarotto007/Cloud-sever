# 机器学习分类

![img](https://github.com/apachecn/ailearning/raw/master/docs/ml/img/ml_add_1.jpg)

# KNN近邻算法

https://github.com/apachecn/ailearning/blob/master/docs/ml/2.md

k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。

k 近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k 近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程。
k 近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。

> KNN 通俗理解

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。

# 决策树

决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。
决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。
决策树学习通常包括 3 个步骤: 特征选择、决策树的生成和决策树的修剪。

## 决策树的定义:

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。

用决策树对需要测试的实例进行分类: 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。

# 朴素贝叶斯

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。

https://github.com/apachecn/ailearning/blob/master/docs/ml/4.md

# Logistic回归

Logistic 回归 或者叫逻辑回归 虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。

# K-Means算法

聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。

K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 `K-均值` 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.

# 知识蒸馏

> # 体系？ 老师学生+教导主任

知识蒸馏是一种模型压缩方法，**是一种基于“教师-学生网络思想”的训练方法**，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作:

https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.02531.pdf

### 2.1. Teacher Model和Student Model

知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:

1. 原始模型训练: 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。
2. 精简模型训练: 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。

在本论文中，作者将问题限定在**分类问题**下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。

### 2.2. 知识蒸馏的关键点

如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): **机器学习最根本的目的**在于训练出在某个问题上泛化能力强的模型。

- **泛化能力强**: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。

而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。

而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。

一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。

**KD的训练过程为什么更有效?**

softmax层的输出，除了正例之外，**负标签也带有大量的信息**，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，**KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式**。

### 2.3. softmax函数

先回顾一下原始的softmax函数:

但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此**"温度"**这个变量就派上了用场。

下面的公式时加了温度这个变量之后的softmax函数:

- 这里的T就是**温度**。
- 原来的softmax函数是T = 1的特例。 T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。

### 3.1. 通用的知识蒸馏方法

- **第一步**是训练Net-T；**第二步**是在高温T下，蒸馏Net-T的知识到Net-S

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-d01f5142d06aa27bc5e207831b5131d9_1440w.webp)

![image-20230721155121303](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230721155121303.png)

# softmax函数

```python
import torch
import torch.nn.functional as F
 
data=torch.FloatTensor([[1.0,2.0,3.0],[4.0,6.0,8.0]])
print(data)
print(data.shape)
print(data.type())
 
prob = F.softmax(data,dim=0) # dim = 0,在列上进行Softmax;dim=1,在行上进行Softmax
print(prob)
print(prob.shape)
print(prob.type())
```

输出为

```python
tensor([[1., 2., 3.],
        [4., 6., 8.]])
torch.Size([2, 3])
torch.FloatTensor
 
tensor([[0.0474, 0.0180, 0.0067],
        [0.9526, 0.9820, 0.9933]])
torch.Size([2, 3])
torch.FloatTensor
```

> # dim = 0 ,在列上进行Softmax， 即 一列之和为1，  softmax函数的输入和输出维度相同

# 神经网络反向传播

https://www.cnblogs.com/charlotte77/p/5629865.html

# 贝叶斯公式

https://zhuanlan.zhihu.com/p/25197792

## 条件概率公式

![image-20231009144504237](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231009144504237.png)

## 贝叶斯公式

假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： **P(患病) = 0.1%**；即：在没有做检验之前，我们预计的患病率为**P(患病)=0.1%**，这个就叫作**"先验概率"**。

再假设现在有一种该病的检测方法，其检测的准确率为**95%**；即：如果真的得了这种病，该检测法有**95%**的概率会检测出阳性，但也有**5%**的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有**95%**的概率检测出阴性，但也有**5%**的概率检测为阳性。用概率条件概率表示即为：**P(显示阳性|患病)=95%**

现在我们想知道的是：在做完检测显示为阳性后，某人的患病率**P(患病|显示阳性)**，这个其实就称为**"后验概率"。**

而这个叫贝叶斯的人其实就是为我们提供了一种可以**利用先验概率计算后验概率**的方法，我们将其称为**“贝叶斯公式”。**

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-e3e7a3aa9fb146d662591612b3cac465_720w.webp)

## 全概率公式

![image-20231009145307892](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231009145307892.png)

# **Model Ensemble（模型集成）**

模型集成在现实中很常用，通俗来说就是针对一个目标，训练多个模型，并将各个模型的预测结果进行加权，输出最后结果。现在的问题是，如何组合这些单模预测结果，来提升最终集成模型的准确性、减少泛化误差。这就是Ensemble的艺术。某些模型可以很好地模拟这些数据的一个方面，而其他模型则可以很好地模拟另一个方面。集成算法提供了一个解决方案，我们可以训练这些模型，并作出合成预测，其中最终的准确度比每个单独的模型好（三个臭皮匠顶个诸葛亮 ）。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-61720a3892457b322af203eae94928b6_720w.webp)

1. **Max Voting**

   最简单的方法：从每个模型中获取最终结果，然后获得每个模型的最大投票权。

   比如下面的3个模型，有2个模型预测结果为1，那final pred就是1（**哪个结果的得票最高，就选择哪个结果，少数服从多数**）

2. **Averaging**

   平均预测通常可以`减少overfit`。理想情况下，希望类之间平滑地分割，而单个模型的预测可能有点粗糙了。

   ![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-0befd9c9c4f5c7371785285b20b26a89_720w.webp)

   上图中，绿线从噪声数据中学习，而黑线有更好的分割效果。平均多条不同的绿线会更接近黑线。

   ==构建机器学习模型的目标不是去理解训练数据，而是很好地概括新的数据，也就是模型要在测试集有较好的泛化能力。==

3. **Weighted Averaging**

   有平均法，自然而然就会有加权平均。`如果一个模型表现好，就给更高的权重`，相反，则权重更低。

4. **Rank averaging**

   上面的方法是加权平均，会发现给模型的权重是人为根据验证集上的评估指标的排序给的（比如M1、M3都给了权重2）。

   现在介绍的方法，是对加权平均的一点改动。

   1. 根据不同模型在验证集上的表现进行排序

      ![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-c9e144e4f155a033f5bd0956cc9d10d5_720w.webp)

   2. 根据排序，计算权重（**R-square / Rank**），这里相当于做了`标准化处理`；

      ![img](https://pic1.zhimg.com/80/v2-8e5568c36dd113a00dd16b98a432cd24_720w.webp)

   3. 模型结果加权求和；

      ![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-210da31bc5a8a17c1368fe9ba7a5137a_720w.webp)

# feed forward network（前馈神经网络）

前馈神经网络（feedforward neural network）是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。**接收前一层的输出，并输出给下一层．各层间没有反馈。**是目前应用最广泛、发展最迅速的人工神经网络之一。研究从20世纪60年代开始，目前理论研究和实际应用达到了很高的水平。

而深度学习模型，类似的模型统称是叫深度前馈网络（Deep Feedforward Network），其目标是拟合某个函数f，由于从输入到输出的过程中不存在与模型自身的反馈连接，因此被称为“前馈”。常见的深度前馈网络有：多层感知机、自编码器、限制玻尔兹曼机、卷积神经网络等等。

## 多层感知机（MLP）

说起多层感知器（Multi-Later Perceptron），不得不先介绍下单层感知器（Single Layer Perceptron），它是最简单的神经网络，包含了输入层和输出层，没有所谓的中间层（隐含层），可看下图：

![img](https://raw.githubusercontent.com/kakarotto007/final/master/ada1101bc4be952e169a75bda5e1c5ec.png)

也就是说，将输入向量赋予不同的权重向量，整合后加起来，并通过激活函数输出1或-1，一般单层感知机只能解决线性可分的问题，如下图：

![img](https://raw.githubusercontent.com/kakarotto007/final/master/7bcf97edf88e6302ba763a5a57285e17.jpg)

我选择了0个隐含层，也就是我们介绍的单层感知机，对于可以线性可分的数据，效果还是可以的。如果我换成线性不可分的数据集，如下图，那么跑半天都跑不出个什么结果来。

[![img](https://raw.githubusercontent.com/kakarotto007/final/master/f1ae5b01408ab4dec59d569ba6faa503.jpg)](https://s2.51cto.com/oss/201911/06/f1ae5b01408ab4dec59d569ba6faa503.jpg)

这个时候就引入多层感知器，它相比单层感知器多了一个隐含层的东西，同样的数据集，我加入两层 隐含层，瞬间就可以被分类得很好。

[![img](https://raw.githubusercontent.com/kakarotto007/final/master/7ec28c2a072ca1d8c65e22c2206a5d4f.jpg)

对于上面直观的了解，我这里还是要深入介绍一下多层感知机的原理。Multi-Layer Perceptron（我们后面都叫MLP），MLP并没有规定隐含层的数量，因此我们可以根据自己的需求选择合适的层数，也对输出层神经元没有个数限制。

# normalization

https://zhuanlan.zhihu.com/p/74516930

**主要区别在于 normalization的方向不同！**

Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize = 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。

而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。

细心的你已经看出来，layer normalization 对所有的特征进行缩放，这显得很没道理。我们算出一行这【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。

那么我们为什么还要使用LN呢？因为NLP领域中，LN更为合适。

如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的**第一个**词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是**针对每个位置**进行缩放，这**不符合NLP的规律**。

而LN则是针对一句话进行缩放的，且L**N一般用在第三维度**，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-5a52774dde73a4dc86bcd55a88be5d04_1440w.webp)

# 三维卷积计算

![带动画效果的卷积神经网络的讲解.pptx_tensorflow_06](https://raw.githubusercontent.com/kakarotto007/final/master/31025300_61ce000c7169c93643.jpg)

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-617b082492f5c1c31bde1c6e2d994bc0_1440w.webp)

# dilated Conv（空洞卷积）

**空洞卷积(Dilated/Atrous Convolution**)

![](https://raw.githubusercontent.com/kakarotto007/final/master/v2-3b7e3ff7c10d01b661752a9b68be7469_1440w.webp)

空洞卷积有什么作用呢？

- **扩大感受野**：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。
- **捕获多尺度上下文信息：**空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。***多尺度信息在视觉任务中相当重要啊。\***

**从这里可以看出，空洞卷积可以任意扩大感受野，且不需要引入额外参数，但如果把分辨率增加了，算法整体计算量肯定会增加。*

![image-20230918160652068](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230918160652068.png)

# 深度可分离卷积

一些轻量级的网络，如mobilenet中，会有深度可分离卷积depthwise separable convolution，由depthwise(DW)和pointwise(PW)两个部分结合起来，用来提取特征feature map

相比常规的卷积操作，其参数数量和运算成本比较低

## 常规卷积操作

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-617b082492f5c1c31bde1c6e2d994bc0_1440w.webp)

卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3。因此卷积层的参数数量可以用如下公式来计算：

N_std = 4 × 3 × 3 × 3 = 108

## 深度可分离卷积

### 逐通道卷积（Depthwise Convolution）

Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积

一张5×5像素、三通道彩色输入图片（shape为5×5×3），Depthwise Convolution首先经过第一次卷积运算，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map(如果有same padding则尺寸与输入层相同为5×5)，如下图所示。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-a20824492e3e8778a959ca3731dfeea3_1440w.webp)

其中一个Filter只包含一个大小为3×3的Kernel，卷积部分的参数个数计算如下：

N_depthwise = 3 × 3 × 3 = 27

Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map

### 逐点卷积（Pointwise Convolution）（1 ✖ 1 卷积）

Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-2cdae9b3ad2f1d07e2c738331dac6d8b_1440w.webp)

由于采用的是1×1卷积的方式，此步中卷积涉及到的参数个数可以计算为：

N_pointwise = 1 × 1 × 3 × 4 = 12

经过Pointwise Convolution之后，同样输出了4张Feature map，与常规卷积的输出维度相同

## **参数对比**

回顾一下，常规卷积的参数个数为：
N_std = 4 × 3 × 3 × 3 = 108

Separable Convolution的参数由两部分相加得到：
N_depthwise = 3 × 3 × 3 = 27
N_pointwise = 1 × 1 × 3 × 4 = 12
N_separable = N_depthwise + N_pointwise = 39

相同的输入，同样是得到4张Feature map，Separable Convolution的参数个数是常规卷积的约1/3。因此，在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。

# 过拟合、欠拟合、完美拟合

通常数据集会被划分成三部分，训练集（training dataset）、验证集（validation dataset）、测试集（test dataset）。我们在训练模型时也经常会根据**训练集的loss和验证集loss**来诊断模型，从而期望能够优化参数训练处一个更好的模型，这个更好指的是能在测试集上表现更好的模型，也就是泛化能力（generalization）强的模型。那怎么根据loss曲线去诊断模型呢？

首先根据模型的表现我们把它分成三类：

- Underfit（欠拟合）
- Overfit（过拟合）
- Good fit （完美拟合）

## 欠拟合

Underfit指的是模型不能很好的学习训练集。

如下图所示，这就是一个Underfit的例子，仅根据training loss就可以判断。这个training loss下降的非常平缓以致于好像都没有下降，这说明模型根本没有从训练集学到什么东西嘛。

![Example of Training Learning Curve Showing An Underfit Model That Does Not Have Sufficient Capacity](https://raw.githubusercontent.com/kakarotto007/final/master/aHR0cHM6Ly8zcWVxcHIyNmNha2kxNmRuaGQxOXN2NmJ5NnYtd3BlbmdpbmUubmV0ZG5hLXNzbC5jb20vd3AtY29udGVudC91cGxvYWRzLzIwMTkvMDIvRXhhbXBsZS1vZi1UcmFpbmluZy1MZWFybmluZy1DdXJ2ZS1TaG93aW5nLUFuLVVuZGVyZml0LU1vZGVsLVRoYXQtRG9lcy1Ob3QtSGF2ZS1TdWZmaWNpZW50LUNhcGFjaXR5LnBuZw)

## 过拟合

Overfit指的是模型把训练集学的有点过了，以致于把一些噪音（noise）和随机波动（random fluctuations）也学进来了。就好像抄别人卷子时候把别人的错别字也照抄一样。这也是我们在训练中最经常出现的问题，overfit有时候是因为训练太久造成的。那Overfit的loss曲线长什么样呢？

如下图所示，overffit时候training loss一直在不断地下降，而validation loss在某个点开始不再下降反而开始上升了，这就说明overfit，我们应该在这个拐点处停止训练。

![Example of Train and Validation Learning Curves Showing an Overfit Model](https://raw.githubusercontent.com/kakarotto007/final/master/aHR0cHM6Ly8zcWVxcHIyNmNha2kxNmRuaGQxOXN2NmJ5NnYtd3BlbmdpbmUubmV0ZG5hLXNzbC5jb20vd3AtY29udGVudC91cGxvYWRzLzIwMTgvMTIvRXhhbXBsZS1vZi1UcmFpbi1hbmQtVmFsaWRhdGlvbi1MZWFybmluZy1DdXJ2ZXMtU2hvd2luZy1Bbi1PdmVyZml0LU1vZGVsLnBuZw)

### 原因

训练数据集太小，过拟合出现的原因：

1. 模型复杂度过高，参数过多
2. 训练数据比较小
3. 训练集和测试集分布不一致
   - 样本里面的噪声数据干扰过大，导致模型过分记住了噪声特征，反而忽略了真实的输入输出特征
   - 训练集和测试集特征分布 或 标签对应关系 不一样（如果训练集和测试集使用了不同类型的数据集会出现这种情况）

### 解决方案

**1、降低模型复杂度**

　　处理过拟合的第一步就是降低模型复杂度。为了降低复杂度，我们可以简单地移除层或者减少神经元的数量使得网络规模变小。与此同时，计算神经网络中不同层的输入和输出维度也十分重要。虽然移除层的数量或神经网络的规模并无通用的规定，但如果你的神经网络发生了过拟合，就尝试缩小它的规模。

**2、增加更多数据**：使用更大的数据集训练模型

**3、数据增强**

　　使用数据增强可以生成多幅相似图像。这可以帮助我们增加数据集规模从而减少过拟合。因为随着数据量的增加，模型无法过拟合所有样本，因此不得不进行泛化。计算机视觉领域通常的做法有：翻转、平移、旋转、缩放、改变亮度、添加噪声等等，[音频数据增强方法](https://www.cnblogs.com/LXP-Never/p/13404523.html)有：增加噪音、增加混响、时移、改变音调和时间拉伸

**4、[正则化](https://blog.csdn.net/u010899985/article/details/79471909?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param)**

　　在损失函数后面加一个正则化项，常见的有L1正则化和L2正则化

| L1正则化                    | L2正则化                        |
| --------------------------- | ------------------------------- |
| 1. L1惩罚权重绝对值的总和   | 1. L2惩罚权重平方和的总和       |
| 2. L1生成简单、可解释的模型 | 2. L2正则化能够学习复杂数据模式 |
| 3. L1受极端值影响较小       | 3. L2受极端值影响较大           |

　　如果数据过于复杂以至于无法准确地建模，那么L2是更好的选择，因为它能够学习数据中呈现的内在模式。而当数据足够简单，可以精确建模的话，L1更合适。对于我遇到的大多数计算机视觉问题，L2正则化几乎总是可以给出更好的结果。然而L1不容易受到离群值的影响。所以正确的正则化选项取决于我们想要解决的问题。

**5、dropout**

　　dropout 是一种避免神经网络过拟合的正则化技术。像L1和L2这样的正则化技术通过修改代价函数来减少过拟合。而丢弃法修改神经网络本身。它在训练的每一次迭代过程中随机地丢弃神经网络中的神经元。当我们丢弃不同神经元集合的时候，就等同于训练不同的神经网络。不同的神经网络会以不同的方式发生过拟合，所以丢弃的净效应将会减少过拟合的发生。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/1433301-20200930170004003-479276499.png)

　　如上图所示，丢弃法被用于在训练神经网络的过程中随机丢弃神经网络中的神经元。这种技术被证明可以减少很多问题的过拟合，这些问题包括图像分类、图像切割、词嵌入、语义匹配等问题。

**6、早停**

![img](https://raw.githubusercontent.com/kakarotto007/final/master/1433301-20200930170321635-1054534217.png)

　　如上图所示，在几次迭代后，即使训练误差仍然在减少，但测验误差已经开始增加了。

**7、重新清洗数据**：把明显异常的数据剔除

**8、使用集成学习方法**：把多个模型集成在一起，降低单个模型的过拟合风险

**9、批量正则化（BN）
**

　　就是将卷积神经网络的每层之间加上将神经元的权重调成标准正态分布的正则化层，这样可以让每一层的训练都从相似的起点出发，而对权重进行拉伸，等价于对特征进行拉伸，在输入层等价于数据增强。注意正则化层是不需要训练。

## 完美拟合

Good fit

Good git是我们的目标，它在loss曲线上的特点是training loss和validation loss都已经收敛并且之间相差很小很小。如下图所示，模型在20轮过后，两个loss曲线都开始收敛，而且两者之间并没有肉眼的差距。 通常traing loss会更小，这样他们之间就会有个gap，这个gap叫做generalization gap

![Example of Train and Validation Learning Curves Showing a Good Fit](https://raw.githubusercontent.com/kakarotto007/final/master/aHR0cHM6Ly8zcWVxcHIyNmNha2kxNmRuaGQxOXN2NmJ5NnYtd3BlbmdpbmUubmV0ZG5hLXNzbC5jb20vd3AtY29udGVudC91cGxvYWRzLzIwMTgvMTIvRXhhbXBsZS1vZi1UcmFpbi1hbmQtVmFsaWRhdGlvbi1MZWFybmluZy1DdXJ2ZXMtU2hvd2luZy1BLUdvb2QtRml0LnBuZw)

# L1正则化 L2正则化

L1正则化会使得权重向量w在优化期间变得稀疏（例如非常接近零向量）。 带有L1正则化项结尾的神经网络仅仅使用它的最重要的并且接近常量的噪声的输入的一个稀疏的子集。相比之下，最终的权重向量从L2正则化通常是分散的、小数字。在实践中，如果你不关心明确的特征选择，可以预计L2正则化在L1的性能优越。

L2正则化也许是最常用的正则化的形式。它可以通过将模型中所有的参数的平方级作为惩罚项加入到目标函数（objective）中来实现，L2正则化对尖峰向量的惩罚很强，并且倾向于分散权重的向量。

# Attention

![Attention的位置](https://raw.githubusercontent.com/kakarotto007/final/master/d3164-2019-11-07-weizhi.png)

之所以要引入 Attention 机制，主要是3个原因：

1. 参数少：模型复杂度跟 [CNN](https://easyai.tech/ai-definition/cnn/)、[RNN](https://easyai.tech/ai-definition/rnn/) 相比，复杂度更小，参数也更少。所以对算力的要求也就更小。

2. 速度快：Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。

3. 效果好：在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。

   Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。

## 原理

Attention 经常会和 [Encoder](https://easyai.tech/ai-definition/encoder-decoder-seq2seq/)–[Decoder](https://easyai.tech/ai-definition/encoder-decoder-seq2seq/) 一起说，之前的文章《[一文看懂 NLP 里的模型框架 Encoder-Decoder 和 Seq2Seq](https://easyai.tech/ai-definition/encoder-decoder-seq2seq/)》 也提到了 Attention。

下面的动图演示了attention 引入 Encoder-Decoder 框架下，完成机器翻译任务的大致流程。

![Attention在Encoder-Decoder框架下的使用](https://raw.githubusercontent.com/kakarotto007/final/master/1ab35-2019-11-13-attention-encoderdecoder.gif)

**但是，Attention 并不一定要在 Encoder-Decoder 框架下使用的，他是可以脱离 Encoder-Decoder 框架的。**

下面的图片则是脱离 Encoder-Decoder 框架后的原理图解。

![attention原理图](https://raw.githubusercontent.com/kakarotto007/final/master/48015-2019-11-13-only-attention.png)

![attention原理3步分解](https://raw.githubusercontent.com/kakarotto007/final/master/efa5b-2019-11-13-3step.png)

第一步： query 和 key 进行相似度计算，得到权值

第二步：将权值进行归一化，得到直接可用的权重

第三步：将权重和 value 进行加权求和

## Attention的类型

![Attention的种类](https://raw.githubusercontent.com/kakarotto007/final/master/7fc4f-2019-11-13-types.png)

### **结构层次**

1）单层Attention，这是比较普遍的做法，用一个query对一段原文进行一次attention。

2）多层Attention，一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。

3）多头Attention，这是Attention is All You Need中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention：

#  Transformer

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031192913762.png" alt="image-20231031192913762" style="zoom: 67%;" />

![image-20231031195232080](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031195232080.png)

1. The
2. 通过词汇表找到对应的id
3. 把一个id（整数）转化为维度为768的token，（embedding）  <img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231031201459202.png" alt="image-20231031201459202" style="zoom:67%;" />

Q、K、V的维度不一定都相同



## one-hot

### one-hot基础

One-Hot编码，又称为**一位有效编码**，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。

假设我们有一群学生，他们可以通过四个特征来形容，分别是：

- 性别：[“男”，“女”]
- 年级：[“初一”，“初二”，“初三”]
- 学校：[“一中”，“二中”，“三中”，“四中”]

举个例子，用上述四个特征来描述小明同学，即“男生，初一，来自二中”，如果特征类别是有序的话，我们能够用表示顺序的数组表示

即“男生，初一，来自一中”   ==>   [0,0,1]

**但是这样的特征处理并不能直接放入机器学习算法中，因为类别之间是无序的。**

这时候就可以用独热编码的形式来表示了，我们用采用N位状态寄存器来对N个状态进行编码，拿上面的例子来说，就是：

| 性别 | [“男”，“女”]                     | N=2  | 男：1 0女：0 1                                       |
| :--- | :------------------------------- | :--- | :--------------------------------------------------- |
| 年级 | [“初一”，“初二”，“初三”]         | N=3  | 初一：1 0 0  初二：0 1 0初三：0 0 1                  |
| 学校 | [“一中”，“二中”，“三中”，“四中”] | N=4  | 一中：1 0 0 0二中：0 1 0 0三中：0 0 1 0四中：0 0 0 1 |

**因此，当我们再来描述小明的时候，就可以采用 [1 0 1 0 0 0 1 0 0]** 

### one-hot编码的作用

之所以使用One-Hot编码，是因为在很多机器学习任务中，**特征并不总是连续值，也有可能是离散值（如上表中的数据）**。将这些数据用数字来表示，执行的效率会高很多。

- 性别：[“男”，“女”]
- 年级：[“初一”，“初二”，“初三”]
- 学校：[“一中”，“二中”，“三中”，“四中”]

若是直接转换成数字的话，[“男”，“初二”，“四中”]的表示方式就是[0,1,3]。

然而，即使转化为数字表示后，上述数据也不能直接用在分类器中。因为分类器往往默认数据数据是连续的、有序的。但是，直接数字并不是有序的，而是随机分配的。为了解决上述问题，其中一种可能的解决方法是采用独热编码。

### one-hot编码的解释

```python
from sklearn import preprocessing  

enc = preprocessing.OneHotEncoder()  
enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])  #这里一共有4个数据，3种特征
array = enc.transform([[0,1,3]]).toarray()  #这里使用一个新的数据来测试

print(array)   # [[ 1  0  0  1  0  0  0  0  1]]
```

这里一共有四个数据，三种特征。是哪四个呢，我们列出矩阵

|        | 第一种 | 第二种 | 第三种 |
| :----- | :----- | :----- | :----- |
| 第一个 | 0      | 0      | 3      |
| 第二个 | 1      | 1      | 0      |
| 第三个 | 0      | 2      | 1      |
| 第四个 | 1      | 0      | 2      |

我们竖着看，可以看出第一种特征中只有0、1两类，第二组有0,、1、2三类，第三种有0、1、2、3四类，因此分别可以用2、3、4个状态类来表示。

`enc.transform`就是将`[0,1,3]`这组特征转换成one hot编码，`toarray()`则是转成数组形式。

第一个数为0，对应第一种特征则为 1 0；

第二个数为1，对应第二种特征则为 0 1 0；

第三个数为3，对应第三种特征则为 0 0 0 1。 

所以最后的输出为：[[ 1 0 0 1 0 0 0 0 1]]

同样的，当我们拿小明的表述[0,0,1]来测试的时候，得到了与第一个例子相同的结果。

```python
from sklearn import preprocessing  
 
enc = preprocessing.OneHotEncoder()  
enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])  #这里一共有4个数据，3种特征
 
array = enc.transform([[0,0,1]]).toarray()  #这里使用一个新的数据来测试
 
print(array)   [[1. 0. 1. 0. 0. 0. 1. 0. 0.]]
```

## seq2seq

![image-20230706202336010](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230706202336010.png)

### 简单介绍

Seq2Seq技术，全称Sequence to Sequence，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型（DNNs）运用于在翻译，文本自动摘要和机器人自动问答以及一些回归预测任务上,并被证实在英语－法语翻译、英语－德语翻译以及人机短问快答的应用中有着不俗的表现。

### 核心思想

Seq2Seq解决问题的主要思路是通过深度神经网络模型（常用的是LSTM，长短记忆网络，一种循环神经网络）http://dataxujing.coding.me/深度学习之RNN/。将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码（Encoder）输入与解码（Decoder）输出两个环节组成, 前者负责把序列编码成一个固定长度的向量，这个向量作为输入传给后者，输出可变长度的向量。![img](https://dataxujing.github.io/seq2seqlearn/img/edpic.jpg)

由上图所示，在这个模型中每一时间的输入和输出是不一样的，比如对于序列数据就是将序列项依次传入，每个序列项再对应不同的输出。比如说我们现在有序列“A B C EOS” （其中EOS＝End of Sentence，句末标识符）作为输入，那么我们的目的就是将“A”，“B”，“C”，“EOS”依次传入模型后，把其映射为序列“W X Y Z EOS”作为输出。

# AST应用到了呼吸音分类上（Patch-Mix）

patch-mix为了解决数据集不足的问题

![image-20230706150542700](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230706150542700.png)

# ImageNet AudioSet

# Fine-tune

***Fine-tuning（微调）\***

## 迁移学习

迁移学习(Transfer learning)  顾名思义就是把已训练好的模型（预训练模型）参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务都是存在相关性的，所以通过迁移学习我们可以将已经学到的[模型参数](https://www.zhihu.com/search?q=模型参数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。

其中，实现迁移学习有以下三种手段：

1. **Transfer Learning**：冻结预训练模型的全部[卷积层](https://www.zhihu.com/search?q=卷积层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，只训练自己定制的全连接层。 
2. **Extract Feature Vector**：先计算出预训练模型的卷积层对所有训练和[测试数据](https://www.zhihu.com/search?q=测试数据&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})的[特征向量](https://www.zhihu.com/search?q=特征向量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，然后抛开预训练模型，只训练自己定制的简配版[全连接网络](https://www.zhihu.com/search?q=全连接网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})。 
3. **Fine-tuning**：冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何网络层，训练剩下的卷积层（通常是靠近输出的部分卷积层）和[全连接层](https://www.zhihu.com/search?q=全连接层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})。

## Fine-tuning原理

Fine-tuning的原理就是利用已知的网络结构和已知的网络参数，修改output层为我们自己的层，微调最后一层前的若干层的参数，这样就有效利用了[深度神经网络](https://www.zhihu.com/search?q=深度神经网络&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})强大的[泛化能力](https://www.zhihu.com/search?q=泛化能力&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})，又免去了设计复杂的模型以及耗时良久的训练，所以fine [tuning](https://www.zhihu.com/search?q=tuning&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})是当数据量不足时的一个比较合适的选择。

## Fine-tuning意义

1. **站在巨人的肩膀上**：前人花很大精力训练出来的模型在大概率上会比你自己从零开始搭的模型要强悍，没有必要重复造轮子。 
2. **训练成本可以很低**：如果采用导出特征向量的方法进行迁移学习，后期的训练成本非常低，用CPU都完全无压力，没有深度学习机器也可以做。 
3. **适用于小数据集**：对于数据集本身很小（几千张图片）的情况，从头开始训练具有几千万参数的大型神经网络是不现实的，因为越大的模型对数据量的要求越大，过拟合无法避免。这时候如果还想用上大型神经网络的超强[特征提取](https://www.zhihu.com/search?q=特征提取&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A798566708})能力，只能靠迁移学习。

# patch

在CNN学习训练过程中，不是一次来处理一整张图片，而是先将图片划分为多个小的块，内核 kernel (或过滤器或特征检测器)每次只查看图像的一个块，==这一个小块就称为 patch==，然后过滤器移动到图像的另一个patch，以此类推。

当将CNN过滤器应用到图像时，它会一次查看一个 patch 。

CNN内核/过滤器一次只处理一个 patch，而不是整个图像。这是因为我们希望过滤器处理图像的小块以便检测特征(边缘等)。这也有一个很好的正则化属性，因为我们估计的参数数量较少，而且这些参数必须在每个图像的许多区域以及所有其他训练图像的许多区域都是“好”的。


# ResNet

随着网络层数的增加，**梯度消失和梯度爆炸**这个问题越来越明显。我们做一个假设，假设每一层的梯度误差是一个小于1的数，在反向传播的过程当中，每向前传播一次，都要乘上一个小于1的误差梯度。当网络越来越深的时候，乘上的小于1 的系数就越来越趋近于0. 这样梯度越来越小，造成了梯度消失的情况。

反过来，梯度是一个大于1的数，在反向传播的过程中，每传播一次梯度就要乘上一个大于1 的数，当网络越来越深的时候，乘上大于1 的系数就无限大，梯度就越来越大，造成梯度爆炸的情况。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/13318-20230221195837027-1428858944.png)

**上面左边**的图主要是针对于网络层数较少（ResNet-34）的使用的残差结构，右边的图是针对层数较多的网络使用的残差结构。
先看左边的结构，主线是输入特征通过两个3×3 的卷积层得到结果，在这个主线的右边是有一个从输入到输出的结构，整个的结构的意思是在主线上经过一系列的卷积层之后得到的特征矩阵再与输入特征矩阵进行一个相加的操作（两个分支的矩阵在相同的维度上做的相加的操作），相加之后通过激活函数输出。注意：主分支与shortcut的输出特征矩阵shape必须是相同的。
 **上面右边**的这个结构，与左边结构的不同是在输入和输出上加了一个1×1的卷积层，这两个1×1的卷积层的作用是用来做什么的呢？
在图中可以知道这个输入矩阵的深度是256-d，通过第一层的卷积层（卷积核是64 ）之后，这个输入矩阵的长和宽是不变的，但是通道数由原来的256变成了64.（第一层的卷积层是起到降维的作用）。到第三层的时候通道数变成了256 ，此时输出和输入的维数是一样的，此时就可以进行相加。

左右两个不同的残差结构对比节省了多少的参数呢？
左边的结构是1,179,648 ，右边结构的参数是69,632，这样看来，使用的残差结构越多，使用的参数就越少。

## ResNet结构

不论是多少层的ResNet网络，它们都有以下共同点：

- 网络一共包含5个卷积组，每个卷积组中包含1个或多个基本的卷积计算过程（Conv-> BN->ReLU）
- 每个卷积组中包含1次下采样操作，使特征图大小减半，下采样通过以下两种方式实现：
  - 最大池化，步长取2，只用于第2个卷积组（Conv2_x）
  - 卷积，步长取2，用于除第2个卷积组之外的4个卷积组
- 第1个卷积组只包含1次卷积计算操作，5种典型ResNet结构的第1个卷积组完全相同，卷积核均为7x7， 步长为均2
- 第2-5个卷积组都包含多个相同的残差单元，在很多代码实现上，通常把第2-5个卷积组分别叫做Stage1、Stage2、Stage3、Stage4

## resnet34

![image-20200611152358539](https://zhoujiahuan.github.io/2020/06/11/ResNet-networks/image-20200805123150470.png)

# pointwise conv（深度可分离卷积的第二步）

Pointwise convolution是一种卷积神经网络中常用的卷积操作。它也被称为1x1卷积，是指卷积核的尺寸只有1x1。与传统的卷积操作不同，传统卷积核的尺寸通常是大于1x1的矩阵，用于在图像或特征图的不同区域进行卷积运算。

Pointwise convolution的操作是在每个像素位置上对特征通道进行独立的卷积运算，而不涉及空间上的邻域。因此，这个操作实际上是一种逐通道的线性变换。例如，对于一个具有C个输入通道的特征图，使用一个尺寸为1x1xC的卷积核，它将对每个通道的像素值进行加权求和，得到一个新的输出通道。可以通过设置多个1x1卷积核的数量来控制输出通道的个数。

Pointwise convolution在卷积神经网络中具有重要作用，它可以实现以下几个功能：

1. 降低通道维度：通过设置输出通道数小于输入通道数，可以在不丢失重要信息的情况下减少计算量和模型参数。
2. 增加通道维度：通过设置输出通道数大于输入通道数，可以引入更多的特征组合，增强模型表达能力。
3. 组合特征：可以使用1x1卷积来组合不同来源的特征图，例如在残差网络中，将两个特征图相加，通过1x1卷积来融合它们。
4. 非线性映射：在1x1卷积后可以添加激活函数，引入非线性映射，增强模型的拟合能力。

Pointwise convolution的效率非常高，因为它仅涉及像素位置的逐通道操作，相比于传统的卷积操作，它具有更少的计算量，因此在很多深度学习模型中得到广泛应用。

# Conformer

![image-20230802170423310](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230802170423310.png)

# workshop

Workshop是一种短期的、以讨论为主的学习活动，通常涉及一个特定的主题或问题。在workshop中，参与者可以分享自己的经验、学习新的技能和知识，并且通过与其他参与者的交流和讨论，加深对所学内容的理解和掌握。

# pooling(池化)

池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效的原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。**池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。**通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每**个输入的特征并减小其大小。**目前最常用形式的池化层是每隔2个元素从图像划分出的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“L2-范数池化”等。

## 下图为最大池化过程的示意图：

![](https://raw.githubusercontent.com/kakarotto007/final/master/1525383043664.jpg)

## 平均池化

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-a47095dd0902990d387e21ae24e6f0b9_1440w.webp)

> **【池化层没有参数、池化层没有参数、池化层没有参数】**

## **池化的作用**

（1）保留主要特征的同时减少参数和计算量，防止过拟合。

（2）invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，我认为这个妥协会越来越小。

现在有些网络都开始少用或者不用pooling层了。

# 一维卷积

所谓一维卷积是指卷积核只在一个方向上移动。具体到语音上，假设一段语音提取特征后是一个M行N列（M表示特征维度，N表示帧数）的矩阵平面，卷积核要在帧的方向上从小到大移动，下图给出了示意。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/1181527-20210206210132866-1440885674.png)

**卷积核也是一个矩阵（J行K列）**。由于卷积核只在一个方向上移动，要把所有的特征值都覆盖到，必须卷积核的行数要等于特征值矩阵的行数，即J = M，所以描述卷积核时只需要知道kernel size（即多少列）和 kernel count（即多少个kernel）。

https://www.cnblogs.com/talkaudiodev/p/14287562.html

# diffusion model （扩散模型）

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-42181e6098a90635a05cfeb1c1091afe_1440w.webp)

# GAN（生成对抗网络）

Generative Adversarial Networks | GAN  （生成对抗网络）

生成对抗网络（GAN）是深度学习的一种创新架构，由Ian Goodfellow等人于2014年首次提出。其基本思想是通过两个神经网络，即生成器（Generator）和判别器（Discriminator），相互竞争来学习数据分布。

- **生成器**：负责从==随机噪声==中学习生成与==真实数据相似==的数据。
- **判别器**：尝试区分生成的数据和真实数据。

两者之间的竞争推动了模型的不断进化，使得生成的数据逐渐接近真实数据分布。

## 应用领域

GANs在许多领域都有广泛的应用，从艺术和娱乐到更复杂的科学研究。以下是一些主要的应用领域：

- **图像生成**：如风格迁移、人脸生成等。
- **数据增强**：通过生成额外的样本来增强训练集。
- **医学图像分析**：例如通过GAN生成医学图像以辅助诊断。
- **声音合成**：利用GAN生成或修改语音信号。

## 生成对抗网络的工作原理

![file](https://raw.githubusercontent.com/kakarotto007/final/master/d4c91b84d9744fbb84da59d8c5f9c4b4%7Etplv-k3u1fbpfcp-zoom-in-crop-mark%3A1512%3A0%3A0%3A0.awebp)

**生成器：**生成器负责从一定的随机分布（如正态分布）中抽取随机噪声，并通过一系列的神经网络层将其映射到数据空间。其目标是生成与真实数据分布非常相似的样本，从而迷惑判别器。

```python
def generator(z):
    # 输入：随机噪声z
    # 输出：生成的样本
    # 使用多层神经网络结构生成样本
    # 示例代码，输出生成的样本
    return generated_sample
```

**判别器：**判别器则尝试区分由生成器生成的样本和真实的样本。判别器是一个二元分类器，其输入可以是真实数据样本或生成器生成的样本，输出是一个标量，表示样本是真实的概率。

```python
def discriminator(x):
    # 输入：样本x（可以是真实的或生成的）
    # 输出：样本为真实样本的概率
    # 使用多层神经网络结构判断样本真伪
    # 示例代码，输出样本为真实样本的概率
    return probability_real
```

## 训练过程

生成对抗网络的训练过程是一场两个网络之间的博弈，具体分为以下几个步骤：

1. **训练判别器**：固定生成器，使用真实数据和生成器生成的数据训练判别器。
2. **训练生成器**：固定判别器，通过反向传播调整生成器的参数，使得判别器更难区分真实和生成的样本。

# squeeze and excitation

![img](https://raw.githubusercontent.com/kakarotto007/final/master/f481041e03b04c2dbe0b7e9653723574%7Etplv-k3u1fbpfcp-zoom-in-crop-mark%3A4536%3A0%3A0%3A0.awebp)

可以看出来，左边的图是一个典型的Resnet的结构，**Resnet这个残差结构特征图求和而不是通道拼接，这一点可以注意一下**

这个SENet结构式融合在残差网络上的，我来分析一下上图右边的结构：

![img](https://raw.githubusercontent.com/kakarotto007/final/master/1578fdc032fb48f08ec7001d7e38a349%7Etplv-k3u1fbpfcp-zoom-in-crop-mark%3A4536%3A0%3A0%3A0.awebp)

>  这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。

# yolo

## 边界框Bounding box

在检测任务中，我们需要同时预测物体的类别和位置，因此需要引入一些跟位置相关的概念。通常使用边界框（bounding box，bbox）来表示物体的位置，边界框是正好能包含物体的矩形框，如 **图1** 所示，图中3个人分别对应3个边界框。

![图1 边界框](https://raw.githubusercontent.com/kakarotto007/final/master/Bounding_Box.png)

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-563f60701e6572b530b7675eabd0cf47_1440w.webp)

# 感受野

![https://www.researchgate.net/publication/316950618_Maritime_Semantic_Labeling_of_Optical_Remote_Sens](https://raw.githubusercontent.com/kakarotto007/final/master/3hnf3pqzdz.png)

# 深度学习过程

train -> validate -> test/predict/inference/estimate

# 训练集、验证集、测试集（分割方法+交叉验证）

先用一个不恰当的比喻来说明3种数据集之间的关系：

- 训练集相当于上课学知识
- 验证集相当于课后的的练习题，用来纠正和强化学到的知识
- 测试集相当于期末考试，用来最终评估学习效果

## 训练集

**训练集（Training Dataset）是用来训练模型使用的。**

## 验证集

当我们的模型训练好之后，我们并不知道他的表现如何。这个时候就可以使用验证集（Validation Dataset）来看看模型在新数据（验证集和测试集是不同的数据）上的表现如何。**同时通过调整超参数，让模型处于最好的状态**。

![验证集使用阶段](https://raw.githubusercontent.com/kakarotto007/final/master/1eb4a-2019-12-20-validationset.png)

验证集有2个主要的作用：

1. 评估模型效果，为了调整超参数而服务
2. 调整超参数，使得模型在验证集上的效果最好

说明：

1. 验证集不像训练集和测试集，它是**非必需的**。如果不需要调整超参数，就可以不使用验证集，直接用测试集来评估效果。
2. 验证集评估出来的效果并非模型的最终效果，主要是用来调整超参数的，模型最终效果以测试集的评估结果为准。

##  测试集

当我们调好超参数后，就要开始「最终考试」了。我们通过测试集（Test Dataset）来做最终的评估。

通过测试集的评估，我们会得到一些最终的评估指标，例如：准确率、精确率、召回率、[F1](https://easyai.tech/ai-definition/accuracy-precision-recall-f1-roc-auc/)等。

## 如何合理的划分数据集？

![如何合理的划分数据集](https://raw.githubusercontent.com/kakarotto007/final/master/2f240-2019-12-20-3type.png)

数据划分的方法并没有明确的规定，不过可以参考3个原则：

1. 对于小规模样本集（几万量级），常用的分配比例是 60% 训练集、20% 验证集、20% 测试集。
2. 对于大规模样本集（百万级以上），只要验证集和测试集的数量足够即可，例如有 100w 条数据，那么留 1w 验证集，1w 测试集即可。1000w 的数据，同样留 1w 验证集和 1w 测试集。
3. 超参数越少，或者超参数很容易调整，那么可以减少验证集的比例，更多的分配给训练集。

## 交叉验证法

**评估模型是否学会了「某项技能」时，也需要用新的数据来评估，而不是用训练集里的数据来评估。这种「训练集」和「测试集」完全不同的验证方法就是交叉验证法。**

**3 种主流的交叉验证法**

- **留出法（Holdout cross validation）**

上文提到的，按照固定比例将数据集**静态的**划分为训练集、验证集、测试集。的方式就是留出法。

- **留一法（Leave one out cross validation）**

每次的测试集都只有一个样本，要进行 m 次训练和预测。 这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。 一般在数据缺乏时使用。

- **k 折交叉验证（k-fold cross validation）**

静态的「留出法」对数据的划分方式比较敏感，有可能不同的划分方式得到了不同的模型。「k 折交叉验证」是一种动态验证的方式，这种方式可以降低数据划分带来的影响。具体步骤如下：

1. 将数据集分为训练集和测试集，将测试集放在一边
2. 将训练集分为 k 份
3. 每次使用 k 份中的 1 份作为验证集，其他全部作为训练集。
4. 通过 k 次训练后，我们得到了 k 个不同的模型。
5. 评估 k 个模型的效果，从中挑选效果最好的超参数
6. 使用最优的超参数，然后将 k 份数据全部作为训练集重新训练模型，得到最终模型。

![k 折交叉验证](https://raw.githubusercontent.com/kakarotto007/final/master/bf13a-2019-12-20-k-fold.png)

**k 一般取 10** 数据量小的时候，k 可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。 数据量大的时候，k 可以设小一点。

# pytorch的MultiheadAttention层

输入维度为 ，形状为 `(seq_length, batch_size, embed_dim)`

\# 创建 MultiheadAttention 层 

`multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)` 

输出的维度是`(seq_length, batch_size, embed_dim)`

如果在`nn.MultiheadAttention`的构造函数中指定了`batch_first=True`，则输入和输出的维度顺序会变为`(batch_size, seq_length, embed_dim)`。

# pytorch的nn.GRU层

输入数据的形状是`（batch_size, seq_len, input_size）`，输出数据的形状是`（batch_size, seq_len, hidden_size）`

\# 创建单层单向GRU模型

`gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True)`

# pytorch的nn.linear层

\# 创建一个全连接层，输入特征数为 64，输出特征数为 128 

`linear_layer = nn.Linear(in_features=64, out_features=128)`

\# 创建随机输入数据，维度为 (batch_size, in_features) 

`input_data = torch.randn(32, 64)`

 \# 输出维度为 (batch_size, out_features)

# pytorch的CNN层

- **输入矩阵**格式：四个维度，依次为：batch size、h、w、d

- **输出矩阵**格式：与输出矩阵的维度顺序和含义相同，但是后三个维度（h、w、d）的尺寸发生变化。

- **权重矩阵**（卷积核）格式：同样是四个维度，为：kernel的长度、kernel的宽度、输入通道数、输出通道数（卷积核个数）

- **输入矩阵、权重矩阵、输出矩阵这三者之间的相互决定关系**

- - 卷积核的输入通道数（in depth）由输入矩阵的通道数所决定。
  - 输出矩阵的通道数（out depth）由卷积核的输出通道数所决定。
  - 输出矩阵的高度和宽度（height, width）这两个维度的尺寸由输入矩阵、卷积核、扫描方式所共同决定。计算公式如下

- ![image-20230811100136248](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230811100136248.png)

![image-20230811100447809](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230811100447809.png)

# pytorch的pooling操作尺寸计算

c尺寸计算与卷积计算公式相同

![image-20230811100136248](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230811100136248.png)

> ## **对于不能整除的数值，卷积向下取整，池化向上取整。**

例子：N = （100-3）/ 2 + 1 = 50。 注意：这里的尺寸不被整除，在池化中是向上取整的，因此97/2 取49 。	

`nn.MaxPool2d` 是 PyTorch 中用于定义二维最大池化层的类。它用于在卷积神经网络（CNN）中进行最大池化操作，以减小特征图的尺寸并保留主要特征。以下是 `nn.MaxPool2d` 构造函数的参数说明：

```python
class nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```

- `kernel_size`（必需）：池化窗口的尺寸。可以是一个整数（对称的正方形池化窗口）或一个元组 `(height, width)`，表示池化窗口的高度和宽度。
- `stride`（可选）：池化操作的步长。可以是一个整数或一个元组 `(height_stride, width_stride)`，表示在输入特征图上滑动的步长。如果未指定，则默认为 `kernel_size`。
- `padding`（可选）：输入特征图的零填充量。可以是一个整数或一个元组 `(height_padding, width_padding)`，默认为 0。
- `dilation`（可选）：池化窗口元素之间的间隔。默认为 1。
- `return_indices`（可选）：如果设置为 `True`，则会返回池化操作过程中的最大值索引，用于后续的反池化操作。默认为 `False`。
- `ceil_mode`（可选）：如果设置为 `True`，则会使用上取整的方式计算输出尺寸，否则使用下取整。默认为 `False`。

`nn.MaxPool2d` 实例化后，可以用于构建卷积神经网络中的最大池化层。最大池化操作会在每个池化窗口内选取最大值作为输出，从而减小特征图的尺寸。这有助于减少网络的参数量和计算复杂度，并且可以在一定程度上保留主要的特征信息。

# hard parameter sharing and soft parameter sharing

多任务学习(multi-task learning, MTL)是指同时学习多个属于不同领域(domain)的任务，并通过特定任务的领域信息提高泛化能力。

与标准的单任务学习相比，多任务学习的方法设计可以分别从网络结构与损失函数两个角度出发。

与标准的单任务学习相比，多任务学习的方法设计可以分别从网络结构与损失函数两个角度出发。

## 多任务学习的网络结构

- 硬参数共享 (Hard Parameter Sharing)：模型的主体部分共享参数，输出结构任务独立。
- 软参数共享 (Soft Parameter Sharing)：不同任务采用独立模型，模型参数彼此约束。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/62dbe42df54cd3f937f34a2a.jpg)

https://0809zheng.github.io/2021/08/28/MTL.html

# mask操作

[mask](https://so.csdn.net/so/search?q=mask&spm=1001.2101.3001.7020)（掩码、掩膜）是深度学习中的常见操作。简单而言，其相当于在原始张量上盖上一层掩膜，从而屏蔽或选择一些特定元素，因此常用于构建张量的过滤器（见下图）。

![在这里插入图片描述](https://raw.githubusercontent.com/kakarotto007/final/master/2020022521234848.png)

按照上述定义，非线性激活函数Relu（根据输出的正负区间进行简单粗暴的二分）、dropout机制（根据概率进行二分）都可以理解为泛化的mask操作。

从任务适应性上，mask在图像和自然语言处理中都广为应用，其应用包括但不局限于：图像兴趣区提取、图像屏蔽、图像结构特征提取、语句padding对齐的mask、语言模型中sequence mask等。

从使用mask的具体流程上，其可以作用于数据的预处理（如原始数据的过滤）、模型中间层（如relu、drop等）和模型损失计算上（如padding序列的损失忽略）。

尽管上述操作均可统称为mask，但其具体的算法实现细节需要根据实际需求进行设计，下面利用pytroch来实现几个典型的例子。

**实例1：图像兴趣区提取**

通过mask张量定义兴趣区（或非兴趣区）位置张量，将非兴趣区上的张量元素用0值填充。

```python
import torch

a = torch.randint(0, 255, (2, 3, 3))   # 两张单通道的图像张量
# tensor([[[ 77,  58, 134],
#         [ 83, 187,  36],
#         [103, 183, 138]],

#        [[223,  19,   7],
#         [ 55, 167,  77],
#         [231, 223,  37]]])

mask = = torch.tensor([[1, 0, 0], [0, 1, 0],  [0, 0, 1]]).bool()   # 1为感兴趣区域位置，0为非感兴趣，可将其值填为0

a.masked_fill_(~mask, 0)      # 注意按照上述定义，需取反~
# tensor([[[ 77,   0,   0],
#         [  0, 187,   0],
#         [  0,   0, 138]],

#        [[223,   0,   0],
#         [  0, 167,   0],
#         [  0,   0,  37]]])

```

**实例2：文本Embedding层中对padding的处理**

在NLP中，虽然CNN特征抽取器要求文本长度为定长，而RNN和Tranformer特征抽取器虽然可应对不定长的文本序列，但为了在batch维度上进行并行化处理，一般还是选择将文本进行长度对齐，即过长序列进行截断，而长度不足序列进行padding操作。padding操作只是数据维度上的对齐，其并不应该对整个网络的计算贡献任何东西，所以必须进行mask操作。

在pytorch中的词嵌入对象Embedding中，直接通过设定padding_idx参数，即可自动对指定的pad编号进行mask操作，即将padding_idx对象的词向量元素均设为0，从而使得该词对网络的正向传播和梯度反向传播均失活，从而达到了mask的目的。
```python
import torch
import torch.nn as nn

a = torch.tensor([[1,2,3], [2,1,0]])   # 2段文本数据
net = nn.Embedding(num_embeddings=10, embedding_dim=5, padding_idx=0)  # 词嵌入层
b = net(a)   # 进行词嵌入
#　tensor([[[-1.8167,  0.0701,  2.0281, -0.7096,  1.0128],
#          [ 2.3647,  1.0678,  0.0383,  0.3265, -0.1237],
#         [ 1.0633,  0.4248,  2.0323, -0.3140, -0.5124]],

#        [[ 2.3647,  1.0678,  0.0383,  0.3265, -0.1237],
#         [-1.8167,  0.0701,  2.0281, -0.7096,  1.0128],
#         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],
#       grad_fn=<EmbeddingBackward>)


# 会发现padding=0处的张量均变为了0

```

```python
import torch
import torch.nn as nn

a = torch.tensor([[1,2,3], [2,1,0]])
net = nn.Embedding(num_embeddings=10, embedding_dim=5)  # 并未设置padding_index参数
a_ = net(a)  # 观察结果，发现果然没有全为0的词向量

mask = (a!=0).float().unsequeeze(-1)  # 手动得到mask张量，注意要升维

b = a_ * mask   # 会发现得到了上述的效果
# tensor([[[ 0.2723, -0.2836,  0.0061,  1.0740, -1.1754],
#         [-0.6695, -0.9080,  0.7244,  0.3022,  0.5039],
#         [ 1.8236, -1.1954, -0.3024,  0.0857, -0.2242]],

#        [[-0.6695, -0.9080,  0.7244,  0.3022,  0.5039],
#         [ 0.2723, -0.2836,  0.0061,  1.0740, -1.1754],
#         [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]],
#       grad_fn=<MulBackward0>)


```

**实例4：self-attention对padding的处理**

Transformer中提出的self-attention机制可以通过文本内部两两元素进行信息的提取，在其中计算各score的softmax权重时，不能将padding处的文本考虑进来（因为这些文本并不存在）。所以在进行softmax前需要进行mask操作（如下图）。

![在这里插入图片描述](https://raw.githubusercontent.com/kakarotto007/final/master/20200225223142594.png)

其具体策略是，将padding的文本处的score设定为负的极大值，这样最终的权重接近于0，从而这些文本不会对attention后生成的向量其作用。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

a = torch.tensor([2,1,0])   # 文本序列
score = torch.tensor([1.2, 2.3, 4])   #　 score结果

mask = (a == 0).bool()　　 # 生成mask
score_ = F.softmax(score.masked_fill(mask, -np.inf), dim=-1)   # padding处填充-inf，从而对softmax结果没影响
# tensor([0.2497, 0.7503, 0.0000])

```

**实例5：序列生成模型中对后续未知文本的处理**
在Transformer中Decoder部分的self-attention处，由于是模型的目的在于序列生成，所以无论在训练过程中需要特意将后续文本进行mask，不让模型看到。

此时的mask操作称为针对序列的sequence mask，其以文本序列长度为单位，生成一个上三角矩阵矩阵，其中上三角元素为0，其余元素为1，表示在当前word处（即矩阵的对角元）模型看不到后面的words。

```python
import numpy as np
import torch 

def sequence_mask(size):
	mask_shape = (size, size)
	return torch.from_numpy(np.tril(np.ones(mask_shape),k=0))

```

Mask机制经常被用于NLP任务中，按照作用总体来说可以分成两类：

1. 用于**处理非定长序列**的padding mask（非官方命名）；
2. 用于**防止标签泄露**的sequence mask（非官方命名）。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/padding%2520mask.png)

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-fe2972a408f2b5f08144bbc885b2f825_720w.jpg)

https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89Mask%E6%9C%BA%E5%88%B6/#xlnet%E4%B8%AD%E7%9A%84mask

# VGG

https://www.jiqizhixin.com/graph/technologies/1d2204cb-e4cf-47c5-bfa2-691367fe2387

# resnet-conformer

https://zhoujiahuan.github.io/2020/06/11/ResNet-networks/

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-235bf2717e609a3a057463d4b4c5a07e_1440w.webp)

https://zhuanlan.zhihu.com/p/548477153

resnet34：

![image-20200611152358539](https://zhoujiahuan.github.io/2020/06/11/ResNet-networks/image-20200805123150470.png)

**2.1关键技术1：ACS数据增强**

针对真实数据稀少的问题，除了利用公开数据集及混响库来仿真数据以外，我们提出了一种音频通道交换(ACS)的数据增强方法，来扩增真实数据方位的多样性。利用麦克风球形排布的特点，通过交换两个通道以及对应的DOA标签，我们可以从已有的数据中获取八倍的数据。ACS增强前后数据的事件检测标签没有发生变化，但是极大地丰富了DOA的排布，使得网络能够更好地进行定位估计。

![](https://pic4.zhimg.com/80/v2-629dedfa5f7a679311c1bb00000b5d87_1440w.webp)

**2.2关键技术2：Conformer长短时上下文同时建模**

在声音事件定位与检测任务中，相当一部分声音事件在长时间内一直存在，而声源的位置可能在帧与帧之间有明显变化，因此对模型的要求是，既可以从长时间的上下文中提取有效信息，又能够兼顾到当前声音信号的局部变化。因此，我们选用了Resnet-Conformer结构作为我们系统的核心模型，其网络结构如图3所示。首先，我们利用了被广泛运用于各种任务的ResNet结构作为提取器，用于从原始的声学特征中提炼出对当前任务有效的高阶特征表示。其次，我们使用了在语音识别领域成功应用的Conformer结构，一方面，其自注意力部分可以捕捉到全局的上下文依赖，另一方面，新增加的卷积部分可以学习到局部关系，因此该模型可以同时利用全局和局部的信息，从而提升了系统的性能。

**2.3关键技术3：时间池化后移**

在通用的技术方案中，原始的声学特征在浅层卷积中做时域降维，因此后端的时序模型得到的往往是较粗分辨率的特征，这在一定程度上会造成信息损失，从而降低模型的性能。为了解决这个问题，我们将时间上的池化操作移到Conformer结构之后，从而提高了网络结构内部的特征分辨率，进一步提高了模型检测和定位准确率。

> ICASSP 2023 有一篇文章

# zero shot(零样本学习)

**任务定义：**
利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集；期间需要借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。

Zero-shot learning 就是希望我们的模型能够对其从没见过的类别进行分类，让机器具有推理能力，实现真正的智能。其中零次（Zero-shot）是指对于要分类的类别对象，一次也不学习。

利用大模型可以拥有zero shot的能力
# One-shot learning --单样本学习 

 Zero-shot learning 指的是我们之前没有这个类别的训练样本。但是我们可以学习到一个映射X->Y。如果这个映射足够好的话，我们就可以处理没有看到的类了。

One-shot learning 指的是我们在训练样本很少，甚至只有一个的情况下，依旧能做预测。这是如何做到呢？可以在一个大数据集上学到general knowledge（具体的说，也可以是X->Y的映射），然后再到小数据上有技巧的update。

One-Shot Learning的意义
1. **减少训练数据**
深度学习需要大量的数据
例：MNIST为了10个类别的区分，需要60000张训练图像，平均一个类别需要6000张训练图像
One-Shot试图将一个类别的训练图像减少，极端情况时只有一张图片
2. **在新类别的数据出现时，无需重新训练**
传统的神经网络无法处理没有出现在训练集中的类别
例：以员工刷脸打卡为例，使用深度神经网络，每一个新员工入职，都是一个类别，需要重新训练深度神经网络。如果每天都有新员工入职，每天都要重新训练网络，成本非常高
One-Shot Learning可以无需重新训练即可应用于新的类别的数据

One-shot learning 属于Few-shot learning的一种特殊情况。

# Few-shot learning -- （小样本学习）

如果训练集中，不同类别的样本只有**少量**，则成为Few-shot learning.

就是给模型待预测类别的少量样本，然后让模型通过查看该类别的其他样本来预测该类别。比如：给小孩子看一张熊猫的照片，那么小孩子到动物园看见熊猫的照片之后，就可以识别出那是熊猫。

Few-shot Learning（少样本学习）是Meta Learning（元学习）中的一个实例，所以在了解什么是Few-shot Learning之前有必要对Meta Learning有一个简单的认识。不过在了解什么是Meta Learning之前还是要了解一下什么是Meta。

## 什么是Meta

meta就是描述数据的数据。

比如照片，我们看到的是它呈现出来的数据， 即Data，但它还含有许多描述它拍摄参数的数据，比如光圈、快门速度、相机品牌等，即Meta。（元数据）

![在这里插入图片描述](https://raw.githubusercontent.com/kakarotto007/final/master/570f45a1199d451e9822a5a791d4bf6a.png)

## meta learning

即让模型“自己学会去学习”。

举个简单的例子，一个小朋友去动物园，里面有些动物他没有见过所以不知道叫什么名字，然后你给他一些小卡片，卡片上有各个动物的==照片和名称==，小朋友就可以自己学习，从这些卡片中找出这些动物的名字。这里的未知动物叫做**query**，小卡片叫做**support set**。培养小朋友从小卡片中自主学习就叫做meta learning。如果一个类别的小卡片只有一张，那么就叫做**one-shot learning**。

小样本带标签的数据集称为support set，由于support set数据样本很少，所以不足以训练一个神经网络。而training set每个类别样本量很大，使用training set训练的模型能够在测试集取得很好的泛化效果。

![在这里插入图片描述](https://raw.githubusercontent.com/kakarotto007/final/master/fb966d7f37db4681aeca4489597493e4.png)

meta learning是从其它学习算法(learning algorithm)的输出中学习，这就要求其它学习算法以及被预训练过。**即meta learning算法将其它机器学习算法的输出作为输入，然后进行回归和分类预测。**

如果说machine learning是使用信息做出更好的预测，那么meta learning就是利用machine learning的预测作出最好的预测。

## Supervised learning vs few-shot learning

**监督学习**

- 测试样本之前从没有见过
- 测试样本类别出现在训练集中

**Few-shot learning**

- query样本之前从没有见过
- query样本来自于未知类别

由于query并未出现在训练集中，我们需要给query提供一个support set，通过对比query和support set间的相似度，来预测query属于哪一类别。

![在这里插入图片描述](https://raw.githubusercontent.com/kakarotto007/final/master/c27f734e357a4ff2a03d66e370a3c006.png)

##  *k-way n-shot* support set

- *k-way*：support set中有个类别
- *n-shot*：每一个类别有个样本

![在这里插入图片描述](https://img-blog.csdnimg.cn/cf91da78d45a4abd9f412cda108cc8f3.png)

## Basic idea behind few-shot learning

Few-shot learning的最基本的思想是学一个相似性函数：s i m ( x , x ‘ ) sim(x,x^{‘})sim(x,x )来度量两个样本x xx和x ’ x^{’}x 
’的相似性。s i m ( x , x ‘ ) sim(x,x^{‘})sim(x,x ‘)越大表明两个图片越相似；越小表明两个图片差距越大。

操作步骤：

从大规模训练数据集中学习相似性函数比较query与support set中每个样本的相似度，然后找出相似度最高的样本作为预测类别。

# 对比学习（contrastive learning）

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-379099fa00fca7e8750bf6abe849d3aa_720w.jpg)

**对比学习**的目标训练一个编码器，此编码器**对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同**。

>对比学习是一种机器学习技术，算法学习区分相似和不相似的数据点。对比学习的目标是学习数据的表示，以捕捉不同数据点之间的基本结构和关系。
>
>在对比学习中，算法被训练最大化相似数据点之间的相似度，并最小化不相似数据点之间的相似度。通常的做法是通过训练算法来预测两个数据点是否来自同一类别。
>
>对比学习已经在各种应用中得到了应用，如图像识别、自然语言处理和语音识别。对比学习的一种流行方法是孪生网络，它使用一对相同的神经网络来学习数据点之间的相似度函数。
>
>总的来说，对比学习是一种强大的技术，可以用于学习数据的表示，并可用于各种下游任务

基本的对比学习框架包括选择一个数据样本，称为“锚点”，一个与锚点属于相同分布的数据点，称为“正”样本，以及另一个属于不同分布的数据点，称为“负”样本样本。

SSL模型试图在潜在空间中最小化anchor和正样本（即属于同一分布的样本）之间的距离，同时最大化anchor和负样本之间的距离。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/6438ef2608f91a7ede8ece37_1.png)

*如上例所示，属于同一类的两幅图像在嵌入空间（“ d+* ”）中彼此靠近，而属于不同类的两幅图像彼此之间的距离较远（“ *d-* ”）。*因此，对比学习模型（在上例中用“ theta* ”表示）试图最小化距离“ *d+* ”并最大化距离“ *d-* ”。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/6438ef43720f0e4ff537bfd0_4.png)

# Semi-Supervised Learning（半监督学习）

半监督学习可进一步划分为**纯（pure）半监督学习**和**直推学习（transductive learning）**。前者假定训练数据中的未标记样本并非待预测的数据，而后者则假定学习过程中所考虑的未标记样本恰是待预测数据。纯半监督学习是基于“开放世界”假设，希望学得模型能适用于训练过程中未观察到的数据，而直推学习是基于“封闭世界”假设，仅试图对学习过程中观察到的未标记数据进行预测。下图直观的表现出**主动学习**、**纯半监督学习**、**直推学习**的区别：

![img](https://raw.githubusercontent.com/kakarotto007/final/master/1539732661652.png)

# VIT（Vision Transformer）

Vision Transformer(ViT) 是一种用于图像识别和分割任务的深度学习模型。它是基于 Transformer 架构的，类似于自然语言处理中使用的 Transformer 模型。但是，与 NLP 中使用的 Transformer 不同的是，ViT 模型**专门用于处理图像数据**。

**把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果**

但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。**CNN具有两种归纳偏置**，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance），
$$
f(g(x))=g(f(x))
$$
其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型

Vision Transformer(ViT) 网络的结构组成包括以下部分:

1. 输入层:ViT 模型接收输入的图像数据，通常是一个多维向量表示。这个向量通常来自于预处理阶段，如裁剪、缩放、归一化等。
2. 编码器：编码器是由多个全连接层组成的神经网络，用于将输入的图像数据映射到一个高维空间中。编码器的最终输出是一个多维向量，代表了图像的不同特征。
3. 注意力层：注意力层是 ViT 模型的核心部分，用于捕捉图像中的局部和全局特征。注意力层由多个全连接层组成，这些全连接层的输出被加权并拼接在一起，形成最终的输出。
4. 解码器：解码器是由多个全连接层组成的神经网络，用于将高维向量映射回原始图像空间。解码器的最终输出是一个多维向量，代表了图像的不同特征。
5. 输出层：输出层是 ViT 模型的最后一层，用于进行分类或分割等任务。输出层的输出是一个二元分类结果或分割结果，通常使用 softmax 函数进行分类或使用 U-Net 等结构进行分割。

![vit.gif](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/05d797f03ec64309810aca50f258afd7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

# TCN（Temporal Convolution Network）

![image-20230918160759647](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230918160759647.png)

下图是 TCN 的一个例子，当 d=1 时，空洞卷积退化为普通卷积。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/v2-87844616e722d229675cc680090d8468_720w.webp)

# Causal Convolution（因果卷积）

![](https://raw.githubusercontent.com/kakarotto007/final/master/v2-b32e451a1f00fd9b0156efe2292303f3_720w.webp)

![image-20230918160558666](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230918160558666.png)

# 欧几里得范数

## L1范数

**L1范数是指向量中各个元素绝对值之和**。

## L2范数

**欧几里得范数（Euclidean norm） == 欧式长度(距离) = L2 范数 == L2距离**==2范数

![](https://raw.githubusercontent.com/kakarotto007/final/master/image-20230927145335419.png)

**||w||上下都有2：意思是二范数的平方**

# 混淆矩阵以及评价指标

![image-20231009143335975](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231009143335975.png)

True Positive（TP）：真正类。样本的真实类别是正类，并且模型识别的结果也是正类。

 False Negative（FN）：假负类。样本的真实类别是正类，但是模型将其识别为负类。

 False Positive（FP）：假正类。样本的真实类别是负类，但是模型将其识别为正类。

 True Negative（TN）：真负类。样本的真实类别是负类，并且模型将其识别为负类。 

- 正确率（查准率 precision） —— 提取出的正确信息条数 / 提取出的信息条数
- 召回率（查全率 recall） —— 提取出的正确信息条数 / 样本中的信息条数
- F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）

![image-20231009143553691](https://raw.githubusercontent.com/kakarotto007/final/master/image-20231009143553691.png)

举个例子如下: 某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只 虾， 100 只乌龟。那么这些指标分别如下: 正确率 = 700 / (700 + 200 + 100) = 70% 召回率 = 700 / 1400 = 50% F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%

# 1乘1卷积

1. 降维或者升维
2. 增加非线性
3. 相当于一个全连接层

# pytorch中的各种乘法

https://juejin.cn/post/7023668955109654559

# permutation issue

<img src="https://raw.githubusercontent.com/kakarotto007/final/master/image-20231101163902936.png" alt="image-20231101163902936" style="zoom:67%;" />

训练阶段： 第一次输入时，女生声音在上面。第二次输入时，女生声音在下面。会使网络崩溃。

# Batch Size

Batch Size定义：一次训练所选取的样本数。

![img](https://raw.githubusercontent.com/kakarotto007/final/master/20171121222637025)

stochastic：batch size = 1

Batch：batch size 等于数据集的个数

mini-batch：。就是说我们选定一个batch的大小后，将会以batch的大小将数据输入深度学习的网络中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。

**为什么要提出Batch Size？**
在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。
在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。

**Batch Size设置合适时的优点：**
1、通过并行化提高内存的利用率。就是尽量让你的GPU满载运行，提高训练速度。
2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。
3、适当Batch Size使得梯度下降方向更加准确。

注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。

Batch Size的正确选择是为了在**内存效率和内存容量**之间寻找最佳平衡。

# Iteration（迭代）

迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以达到所需的目标或结果。

每一次迭代得到的结果都会被作为下一次迭代的初始值。

**一个迭代=一个正向通过+一个反向通过。**

Iteration是batch需要完成一个epoch的次数。

有一个2000个训练样本的数据集。将2000个样本分成大小为500的batch,那么完成一个epoch需要4个iteration。

# epoch（时期）

一个时期= ==所有== 训练样本的一个**正向传递**和一个反向传递。

（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；

### 为什么要使用多于一个epoch?

在神经网络中**传递完整的数据集一次是不够的**，而且我们需要将完整的数据集在同样的神经网络中传递多次。但请记住，我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降来优化学习过程。如下图所示。因此仅仅更新一次或者说使用一个epoch是不够的。

# 深度学习的优化算法

深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。

https://blog.csdn.net/S20144144/article/details/103417502
